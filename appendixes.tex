
\appendix

\section{Architecture}
\label{app:arch}

This appendix provides additional technical details for the architecture introduced in \S\appref{sec:tabstar}. First, we discuss the verbalization module; next, we formally describe the architecture step-by-step; and finally, we present selected experiments on the TabSTAR architecture.

\subsection{The Verbalization Module}
\label{app:arch:verbalization}

TabSTAR's verbalization module standardizes heterogeneous tabular inputs by converting each column, whether predictive feature or target variable, into templated text blocks. We first describe the detection of column types, and then detail the processing steps for each type.

\paragraph{Feature Detection}  
We classify each column as either numerical, referring to quantitative values, or semantic, referring to textual values including categorical and boolean fields encoded as text. We rely on heuristics involving both the primitive data type (e.g., string, float) and human annotation (e.g., OpenML metadata). However, real-world datasets pose challenges, as numerical features can often be stored as strings (e.g., ``35 years'', ``unknown age'') or may lack inherent order (e.g., country calling codes). Leveraging LLMs for contextualized data cleaning can be a promising direction \cite{bendinelli_exploring_2025}.

A special case is the handling of timestamp and date columns. Similarly to \cite{hoo_tabular_2024}, we rely on \textit{skrub's DatetimeEncoder\footnote{https://skrub-data.org/}} to detect datetime columns and decompose each one of them into a set of new features. Each extracted feature then undergoes its own processing: For example, the weekday is treated as semantic, while the total seconds since the Unix epoch is treated as numerical. Integrating date features more holistically remains an open research question.


\begin{table}[h]
\centering
\caption{Illustrative verbalization of a numerical feature (\textit{Age}) with 10 bins. Examples outside the range and missing values are considered as special bins.}
\begin{tabular}{rlll}
\toprule
\textbf{Bin} & \textbf{Range}        & \textbf{Example Value} & \textbf{Illustrative Verbalization}                          \\
\midrule
--  & Lower than 18      & 17                     & Age: Lower than 18 (Quantile 0\%)            \\
1  & 18–23              & 20                     & Age: 18–23 (Quantile 0–10\%)                     \\
2  & 23–27              & 25                     & Age: 23–27 (Quantile 10–20\%)                    \\
3  & 27–31              & 29                     & Age: 27–31 (Quantile 20–30\%)                    \\
4  & 31–35              & 33                     & Age: 31–35 (Quantile 30–40\%)                    \\
5  & 35–40              & 38                     & Age: 35–40 (Quantile 40–50\%)                    \\
6  & 40–45              & 42                     & Age: 40–45 (Quantile 50–60\%)                    \\
7  & 45–51              & 48                     & Age: 45–51 (Quantile 60–70\%)                    \\
8  & 51–58              & 55                     & Age: 51–58 (Quantile 70–80\%)                    \\
9  & 58–67              & 63                     & Age: 58–67 (Quantile 80–90\%)                    \\
10  & 67–87              & 83                     & Age: 67–87 (Quantile 90–100\%)                    \\
-- & Higher than 87     & 93                     & Age: Higher than 87 (Quantile 100\%)          \\
-- & Unknown            & –                      & Age: Unknown Value                               \\
\bottomrule
\end{tabular}
\label{tab:numerical_bin_verbalization}
\end{table}

\paragraph{Numerical Features}  
Numerical features are represented by both a numerical and a semantic representation. For the numerical representation, given a value $x$, we compute the clipped z-score $z' = \operatorname{clip}\bigl((x - \mu)/\sigma,\,-3,\,3\bigr)$ where $\mu,\sigma$ are the training set mean and the standard deviation, and missing values are set to 0. For the semantic representation, we build $B=10$ quantile bins over the training distribution to map the value accordingly. Table~\ref{tab:numerical_bin_verbalization} shows an illustrative example for the feature \textit{Age} from our running example in Table~\appref{tab:patient_verbalization}. 



\paragraph{Semantic Features}\label{app:arch:verbalize_semantic}  
Semantic features are sanitized (e.g., normalizing whitespaces) and verbalized using the template presented in Table~\ref{tab:verbalization_templates}. Missing values are mapped to ``Unknown Value'', just like for numerical features. If a text exceeds the model's context window (512 tokens for \textit{e5-small-v2}), it is naively truncated to fit it. This limitation is far more pronounced for methods that serialize the entire example into a single textual sequence \cite{yan_making_2023}, thereby dramatically reducing the effective context size.


\paragraph{Target Variables} The verbalization templates for the target values are prepended to every example. For classification tasks, each possible label is verbalized, while for regression we verbalize a single element consisting solely of the feature name. Employing a binning strategy to treat regression as a classification task is a future work direction, as discussed in \S\appref{sec:discussion}. For regression tasks, target values go through the same standardization with outlier clipping as numerical features, being used solely as the ground truth without going through the input. 


\begin{table}[h]
\centering
\caption{Verbalization templates for semantic features and target values.}
\begin{tabular}{p{0.4\linewidth}p{0.5\linewidth}}
\toprule
\textbf{Element Type} & \textbf{Verbalization Template} \\
\midrule
Predictive feature      & "Predictive Feature: \{feature\_name\}" \\
                        & "Feature Value: \{feature\_value\}" \\[1ex]
Classification target   & "Target Feature: \{target\_name\}" \\
                        & "Feature Value: \{target\_value\}" \\[1ex]
Regression target       & "Numerical Target Feature: \{target\_name\}" \\
\bottomrule
\end{tabular}
\label{tab:verbalization_templates}
\end{table}

\subsection{The Annotated TabSTAR}

Table~\ref{tab:param_counts} describes the number of parameters per component in the TabSTAR architecture, when using \textit{e5-small-v2} \cite{wang_text_2024} as the text encoder. It has approximately 47.26M parameters, most of which come from the text encoder. When unfreezing 6 layers of the text encoder, about 24.70M parameters are tuned, with the remaining 11.92M embedding parameters and 10.65M layer ones being kept frozen. 



\begin{table}[h]
\centering
\caption{Parameter counts for TabSTAR components}
\begin{tabular}{l r}
\toprule
\textbf{Module} & \textbf{\# Parameters} \\
\midrule
Encoding: Semantic   & 33,360,000 \\
Encoding: Numerical                &    296,832 \\
Fusion                      &  1,774,464 \\
Interaction & 10,646,784 \\
Prediction  &  1,185,794 \\
\bottomrule
\end{tabular}
\label{tab:param_counts}
\end{table}

To describe the architecture more precisely, we start by defining the dataset formally. Let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ denote a tabular dataset with $n$ examples.  Each example $x_i = [x_{i1}, \dots, x_{im}]$ has $m$ features. The target variable $y_i$ is either continuous (regression) or discrete (classification) taking one of $C$ classes.  For simplicity, we describe the architecture at the example level, though all computations are actually carried out on mini-batches of size $B$. The batches are always drawn from a single dataset in both pretraining and finetuning, removing any need for padding.

\paragraph{Verbalization}  
We denote by $t$ the number of target entries where $t=C$ for classification and $t=1$ for regression. We then form a raw sequence of length $e = t + m$ by listing the $t$ target values, followed by the $m$ feature entries. Each element $j$ in this sequence is then verbalized into a semantic string $s_{j}$ and a numerical value $n_{j}$, set to be the clipped z-score for numerical non-missing features, and zero otherwise. The example is thus represented by parallel sequences $(\mathbf{s}, \mathbf{n})$ of length $e$.  

\paragraph{Encoding} Each semantic string $s_j$ and numerical value $n_j$ are projected into a $d$-dimensional vector. Semantic strings are encoded with an encoder-only language model (\textit{e5-small-v2 \cite{wang_text_2024}}). Each string is tokenized, passed through the model, and pooled to produce its final embedding. This process is independent between elements, i.e. the attention is at the token level within a single element. In parallel, each numeric value is fed through a two-layer MLP that first projects from 1 to $2d$ dimensions, applies a ReLU and dropout, and then projects back down to $d$. This produces matching $d$-dimensional embeddings for each of the $e$ elements, ready to be fused.

\paragraph{Fusion}  
To unify semantic and numerical embeddings into a single representation, we apply a single-layer Transformer Encoder\footnote{With 2 attention heads, a feed-forward hidden size of $4d$, dropout 0.1, and ReLU activation.} over each element's pair of vectors. Concretely, for each element we stack its $d$-dimensional text and numeric embeddings and feed them through the encoder layer. For every element, the attention is applied between its two representations, and we average the two outputs to produce one fused $d$-dimensional embedding. This yields a final sequence of length $e$ and dimension $d$, which will serve as tokens for the Interaction block.

\paragraph{Interaction}  
The fused sequence of $e$ tokens is processed by a standard Transformer Encoder with model dimension $d=384$, $L=6$ layers, $6$ attention heads per layer, feed‐forward size $4d$, dropout $0.1$, ReLU activation and using a pre-norm configuration. Unlike in language modeling, feature ordering is irrelevant, so no positional encodings are used. The encoder produces contextualized embeddings for every position, and we retain the $t$ target embeddings for the prediction.


\paragraph{Prediction} We initialize two identical MLP heads, one for regression and one for classification. Each of them consists of a hidden layer of size $4d$ (with ReLU activation) followed by a linear projection to a single output. For each dataset, we choose the relevant head and process the $t$ target token embeddings. For classification, we independently feed each one of the $t=C$ target tokens to the classification head to obtain a score (logit) per class. Notably, the same head is shared across classes and datasets. We apply softmax over these scores, yielding a probability distribution regardless of the number of classes. For regression, the single target token is projected into a real value. Note that the heads is shared between datasets, as regression outputs are always clipped z-scores. 

\subsection{Architecture Experiments}\label{app:arch:ablation}

In this section we explore the effect of different design choices for TabSTAR's architecture. For each experiment, we only vary the parameter of interest, keeping everything else fixed. We follow the same pretraining regime as in Appendix~\ref{app:pretrain}, except that for computational efficiency we train only 25 epochs (instead of 50) with 128 pretraining datasets (instead of 390). We evaluate each variant relying solely on pretraining performance, as an approximation for downstream task performance. We acknowledge that our conclusions might depend on this limited scale, hence we discuss a subset of the experiments briefly to reflect the depth of our work and inspire future research.

\paragraph{The Fusion Block's Mechanism} For the fusion block, we consider two simpler alternatives to the attention mechanism, both of then underperforming: (1) Concatenation, by concatenating the semantic and numerical $d$-dimensional vectors into a $2d$-dimensional vector, and projecting them back via an MLP, and (2) Multiplication, by multiplying the semantic representation directly with the numerical value\footnote{After rescaling it to be centered around 1 rather than 0, using a learned scaling factor.} in a straightforward, parameter‐free manner as in \cite{wang_transtab_2022, ye_towards_2024}.

\paragraph{The Number of Interaction Layers} We experiment with the number of encoder layers, and observe that 3 yields anecdotally worse performance than 6, with lower parameter count. Nevertheless, we prioritize a deeper network as for datasets with very complex relationships we believe that this might be beneficial. Additionally, we try a 9-layer variant which performs significantly worse, while also increasing the parameter count. 

\paragraph{Row-Level Attention}\label{app:arch:saint} We experiment with adopting the architecture proposed by SAINT \cite{somepalli_saint_2021}, which adds row-level attention to each encoder layer. Similar concepts are also employed by models that get the input the whole dataset, labels included \cite{hollmann_accurate_2025, kossen_self-attention_2021}. We run experiments with 2, 4 and 6 layers as they are roughly equivalent in parameter count to 3, 6, and 9 layers without row-attention. We observe no substantial gain, and thus we prioritize the simpler solution, as row-level attention is sensitive to the batch size and adds complexity to inference time.


\section{Training}
\label{app:train}

In this section we elaborate on the two stages of TabSTAR's training: pretraining and finetuning, presented in \S\appref{sec:tabstar}. As in Appendix~\ref{app:arch:ablation}, we summarize key pretraining experiments.

\subsection{Pretraining}
\label{app:pretrain}

TabSTAR is pretrained employing supervised learning in a multi-task regime, jointly learning regression, binary and multiclass classification tasks. The parameters of the architecture are fully-shared, without any need for dataset-specific parameters. Every example during the pretraining updates all the model's weights, with the sole exception of the prediction head, for which every example uses its respective head depending on the nature of the task (classification or regression).

\paragraph{Sampling} For computational efficiency, each dataset is subsampled once before the pretraining. At the example level, we sample up to 300,000 examples from each dataset, stratified by the target variable for classification tasks. Since we only use a fraction of each dataset for each pretraining epoch, this decision has negligible influence. In addition, we randomly sample up to 200 features per dataset. While straightforward, this decision is suboptimal as feature importance isn't taken into consideration. As this work does not focus on wide-feature datasets, we consider this trade-off acceptable. Importantly, this setup is enabled during finetuning as the TabSTAR architecture is agnostic to the number of features. We split each dataset into train-validation splits (95\%-5\%),\footnote{We choose only 5\% for efficiency, as we use hundreds of pretraining datasets.} without any need for test splits, and cap the validation set at a maximum of 1,000 examples used for evaluating pretraining performance.

\paragraph{Batching} Every epoch, we randomly sample up to 2,048 examples from each dataset in mini-batches of 32, and shuffle all the batches. We conduct gradient accumulation and update the model every 4 steps to reduce the chances of a single update being dominated by a single dataset, so the global batch size is effectively 128. Appendix~\ref{app:exp_batch_size} elaborates on the effect of batch size.

\paragraph{Metrics} Our loss function is cross-entropy for classification, and MSE for regression. With standardized targets, \( R^2 \approx 1 - \text{MSE} \), although this equivalence is degraded by clipping targets in preprocessing. We train with mixed-precision and apply gradient clipping to stabilize training without task-specific weights, with Appendix~\ref{app:exp_weights} discussing the limitations of this approach. We use as metrics AUROC for classification and \(R^2\), so for each task the optimal metric value is 1. We average performance across all datasets into a single metric that reflects the pretraining performance.


\paragraph{Training} We pretrain for 50 epochs with the \textit{OneCycleLR} \cite{smith_super-convergence_2018} optimizer, with warmup during the first 5 epochs (10\%) and cosine annealing. Early stopping is conducted after 3 epochs without improvement on the pretraining metric. The weight decay is set to 0.001, and a max learning rate of $lr=5 \times 10^{-5}$ is applied uniformly across all layers. Appendix~\ref{app:exp_lr} discusses experiments with differential learning rate.
Pretraining running time varies depending on the number of epochs and the included datasets. The full models (390 datasets) reported in \S\appref{sec:results} train for less than 48 hours on a single NVIDIA A40 GPU (48GB memory), and we believe that this could be optimized much further.


\subsection{Finetuning}
\label{app:lora}

We finetune downstream tasks using LoRA's implementation of the \textit{peft} package\footnote{https://github.com/huggingface/peft}. We use a rank of $r=32$, set $\alpha=2r=64$ and $dropout=0.1$. We employ the same scheduler as in the pretraining phase, with the only difference being that we set $lr=0.001$, and increase the patience parameter for early stopping to 5. We apply a train-test split of 90\%-10\% and sample a validation set of 10\%. As opposed to the pretraining, all batches are drawn from the same dataset. Therefore, we observe no effect from changing the mini-batch size when keeping the global batch size fixed to 128. We tune 1,597,440 out of TabSTAR's 47,263,874 parameters (3.4\%).

Finetuning hyperparameters are selected by pretraining TabSTAR over 256 datasets and performing grid-search over a held-out set of 25 downstream tasks disjoint from the 50 datasets in the benchmark evaluated in \S\appref{sec:exp:benchmark}. The search space is presented in Table~\ref{tab:lora_hyperparam_grid}, and we observe that average performance is relatively robust across this space. An interesting observation is that decreasing the number of parameters by setting $r=16$ mildly hurts performance, but it has upsides on memory and latency aspects, allowing a future trade-off exploration. As a final note, we argue that providing a strong default configuration for TFMs is crucial for evaluating them, but for real-world applications, it is still recommendable to find the best hyperparameters tailored to the downstream task. 

\begin{table}[h]
\centering
\caption{LoRA hyperparameter tuning grid search for TabSTAR's finetuning.}

\begin{tabular}{ll}
\toprule
\textbf{Hyper-parameter} & \textbf{Search Space} \\
\midrule
LoRA rank ($r$)            & 16, 32, 64     \\
Learning Rate              & 0.0005, 0.001, 0.002, 0.005, 0.01 \\
Dropout                     & 0, 0.1     \\
\bottomrule
\end{tabular}
\label{tab:lora_hyperparam_grid}
\end{table}

The only experiment in this paper where we employ full finetuning instead of LoRA is for the non-pretrained variant discussed in the analysis in \S\appref{analysis:scaling}. For this variant we fully finetune the pretrained model on each downstream task. Compared to the pretraining, we use $lr=2.5 \times 10^{-5}$ and increase the patience to 5. These hyperparameters are lightly tuned using the same procedure as for LoRA, and we observe that fully finetuning the model achieves comparable performance, except for small datasets, where training is more prone to overfitting.

\subsection{Pretraining Experiments}
\label{app:arch_exp}

In this section, we briefly elaborate on some experiments performed over TabSTAR's pretraining protocol. As in Appendix~\ref{app:arch:ablation}, we highlight only a subset of them in a high-level manner.

\subsubsection{Batch Size}\label{app:exp_batch_size}

During pretraining, we use a mini-batch size of 32, each of them drawn from a single dataset. Since we train with gradient accumulation and a global batch size of 128, varying the batch size affects the diversity of a single model update: lower batch sizes are likely to be exposed to more datasets. We decrease the batch size to 16 and 8 and observe an improvement at the cost of slower training. An interesting direction for future work is moving to mixed-datasets batches, which require more complex implementation but might benefit from more regularized learning. Such approach, however, goes against row-level attention methods and ICL, as discussed in Appendix~\ref{app:arch:saint}.

\subsubsection{Loss Weights}\label{app:exp_weights}

Pretraining the model over hundreds of datasets in a multi-task regime presents a key challenge: the loss scale of each dataset can vary substantially, depending on task difficulty or task type. For example, a multiclass classification task with dozens of classes will naturally yield a higher average loss than a binary task. These dynamics can also shift during training. Our default approach naively averages the loss across all datasets, which risks over-weighting tasks for potentially arbitrary reasons.

To address this, we explore two alternative weighting strategies: (1) Assigning a constant weight per task type, accounting for the number of classes in classification tasks, and (2) Normalizing each dataset's contribution by the best loss achieved by CatBoost \cite{prokhorenkova_catboost_2018} when being fitted to that dataset. While these strategies better reflect task-specific characteristics, they hardly impact performance and introduce additional complexity. Notably, adjusting loss weights across tasks impacts metric interpretability, as each weighting scheme implicitly optimizes a different objective. 

We do not explore more sophisticated methods such as learning per-dataset weights, as these often require mixed-dataset batches and introduce additional learnable parameters. We believe, however, that multi-task pretraining over tabular datasets remains an open and important research question.


\subsubsection{Differential Learning Rate}\label{app:exp_lr}

TabSTAR's weights initialization is not balanced: the textual encoder is a pretrained embedding model while the rest of the architecture parameters are randomly initialized. To counteract this imbalance, we experiment with using differential learning rates for the textual encoder layers, and experiment with scaling it by a factor of 0.5 and of 0.75. To our surprise, this decision hurts performance, so we stick to a uniform learning rate across all layers.


\section{Training Datasets}
\label{app:train_datasets}

In this appendix we briefly expand on the pretraining corpus elaborated in \S\appref{sec:exp:corpus}. It is composed of 350 datasets, spanning all the datasets appearing in \textit{AMLB} \cite{gijsbers_amlb_2024}, \textit{OpenML-CTR23} \cite{fischer_openml-ctr23_2023}, \textit{TabZilla} \cite{mcelfresh_when_2023} and the ones presented by \textit{Grinsztajn} \cite{grinsztajn_why_2022}. After deduplication,\footnote{And the exclusion of the \textit{fifa} dataset, which is included in the benchmark.} this results in 152 datasets (94 classification, 58 regression). Interestingly, only 6 of these 152 datasets have free-text or high-cardinality features. We manually add datasets from OpenML \cite{vanschoren_openml_2014} and Kaggle, as well as from the \textit{AutoML-Benchmark-Train} \cite{feurer_auto-sklearn_2022} corpus, and achieve a total of 350 datasets, with 49 textual datasets.

Table~\ref{tab:cls_pretrain_metadata} details the 253 classification datasets and Table~\ref{tab:reg_pretrain_metadata} the 97 regression ones. We elaborate the \textit{Dataset} name, the number of examples $n$, the number of features $m$, and the number of classes $C$ for classification. In addition, we mark datasets that belong to one of the benchmarks, and the ones that have text features. Importantly, the textual flag is quite permissive, as it includes features with relatively short texts or potentially low predictive power (e.g., people names or addresses).


\makeatletter
\setlength\LTcapwidth{\textwidth}
\makeatother
\begin{longtable}{llllll}
\caption{The 253 Classification Datasets of the Pretraining Corpus, with their $n$ examples, $m$ features, $C$ classes, presence in a benchmark ($B$) and whether they are textual ($T$).} \label{tab:cls_pretrain_metadata} \\
\toprule
Dataset & $n$ & $m$ & $C$ & B & T \\
\midrule
\endfirsthead
\caption[]{The 253 Classification Datasets of the Pretraining Corpus.} \\
\toprule
Dataset & $n$ & $m$ & $C$ & B & T \\
\midrule
\endhead
\midrule
\multicolumn{6}{r}{Continued on next page} \\
\midrule
\endfoot
\bottomrule
\endlastfoot
\href{https://www.openml.org/search?type=data&id=42746}{KDDCup99} & 4,898,422 & 40 & 20 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46678}{mimic\_extract\_los\_3} & 4,155,270 & 17 & 68 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43502}{Online-P2P-Lending} & 2,875,146 & 16 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=42732}{sf-police-incidents} & 2,215,023 & 8 & 2 & \checkmark & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46677}{physionet\_sepsis} & 1,552,210 & 42 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1567}{poker-hand} & 1,025,009 & 10 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42769}{Higgs} & 1,000,000 & 28 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46646}{BAF\_base} & 1,000,000 & 30 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=45955}{Credit\_Card\_Fraud\_} & 1,000,000 & 7 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43544}{Harry-Potter-fanfiction-data} & 648,493 & 13 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=42742}{porto-seguro} & 595,212 & 57 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1596}{covertype} & 581,012 & 54 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46648}{AVIDa-hIL6} & 573,891 & 3 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=1169}{airlines} & 539,383 & 7 & 2 & \checkmark & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46684}{HolisticBias} & 472,991 & 14 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41147}{albert} & 425,240 & 78 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46686}{DBPedia} & 342,781 & 3 & 219 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=45567}{hcdr\_main} & 307,511 & 120 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46721}{Mental\_Health\_Dataset} & 292,364 & 16 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=41991}{Kuzushiji-49} & 270,912 & 784 & 49 &  &  \\
\href{https://www.openml.org/search?type=data&id=1503}{spoken-arabic-digit} & 263,256 & 14 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=46598}{cdc\_diabetes} & 253,680 & 21 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1502}{skin-segmentation} & 245,057 & 3 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46430}{LT-Vehicle-Loan-Default-Prediction} & 233,154 & 38 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=44228}{Churn\_Telco\_Europa} & 190,776 & 17 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1483}{ldpa} & 164,860 & 6 & 11 &  &  \\
\href{https://www.openml.org/search?type=data&id=45577}{Give-Me-Some-Credit} & 150,000 & 10 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1509}{walking-activity} & 149,332 & 4 & 22 &  &  \\
\href{https://www.openml.org/search?type=data&id=46701}{social\_bias\_frames} & 144,649 & 16 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46708}{Wikipedia\_Talk\_Labels} & 140,379 & 12 & 15 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43838}{Municipal-Debt-Risk-Analysis} & 138,509 & 13 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41150}{MiniBooNE} & 130,064 & 50 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42806}{nba-shot-logs} & 128,069 & 15 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46674}{college\_scorecard} & 124,699 & 117 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43044}{drug-directory} & 120,215 & 16 & 7 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43743}{TVS\_Loan\_Default} & 119,528 & 29 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=45038}{road-safety} & 111,762 & 32 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=4541}{Diabetes130US} & 101,766 & 46 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40672}{fars} & 100,968 & 29 & 8 &  &  \\
\href{https://www.openml.org/search?type=data&id=46441}{Credit\_Score\_Classification} & 100,000 & 26 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=23517}{numerai28.6} & 96,320 & 21 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40922}{Run\_or\_walk\_information} & 88,588 & 6 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41168}{jannis} & 83,733 & 54 & 4 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42343}{KDD98} & 82,318 & 477 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41138}{APSFailure} & 76,000 & 169 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41162}{kick} & 72,983 & 32 & 2 & \checkmark & \checkmark \\
\href{https://www.kaggle.com/eilamshapira/human-choice-prediction-in-language-based-games/OPE_train.csv}{human-choice-prediction} & 71,579 & 20 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=42345}{Traffic\_violations} & 70,340 & 20 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=40996}{Fashion-MNIST} & 70,000 & 784 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=45547}{Cardiovascular-Disease-dataset} & 70,000 & 11 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=554}{mnist\_784} & 70,000 & 719 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=40668}{connect-4} & 67,557 & 42 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44231}{mobile\_churn} & 66,469 & 63 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41169}{helena} & 65,196 & 27 & 100 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46640}{LICD} & 63,634 & 413 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=40927}{CIFAR\_10} & 60,000 & 3,072 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=46681}{REASONER} & 58,497 & 34 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41166}{volkert} & 58,310 & 147 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40685}{shuttle} & 58,000 & 9 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41990}{GTSRB-HueHist} & 51,839 & 256 & 43 &  &  \\
\href{https://www.openml.org/search?type=data&id=42734}{okcupid-stem} & 50,789 & 19 & 3 & \checkmark & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43072}{KDDCup09-Upselling} & 50,000 & 13,419 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1111}{KDDCup09\_appetency} & 50,000 & 207 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1590}{adult} & 48,842 & 14 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43635}{League-of-Legends-Diamond} & 48,651 & 14 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40985}{tamilnadu-electricity} & 45,781 & 2 & 20 &  &  \\
\href{https://www.openml.org/search?type=data&id=1461}{bank-marketing} & 45,211 & 16 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=279}{meta\_stream\_intervals} & 45,164 & 74 & 11 &  &  \\
\href{https://www.openml.org/search?type=data&id=41027}{jungle\_chess} & 44,819 & 6 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46683}{Dynamically-Generated-Hate-Speech-Dataset} & 41,144 & 8 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43687}{Breast-cancer-prediction} & 39,998 & 11 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=42733}{Click\_prediction\_small} & 39,948 & 11 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43721}{Hotel-Reviews} & 38,932 & 3 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44156}{electricity} & 38,474 & 8 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1486}{nomao} & 34,465 & 118 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43551}{Employee-Turnover-at-TECHCO} & 34,452 & 9 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=4135}{Amazon\_employee\_access} & 32,769 & 9 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43454}{Credit-Risk-Dataset} & 32,581 & 11 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43435}{Default-of-Credit-Card-Clients-Dataset} & 30,000 & 23 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46692}{funpedia} & 29,819 & 3 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46444}{credit\_risk\_china} & 27,522 & 27 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=45064}{Insurance} & 23,548 & 10 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41159}{guillermo} & 20,000 & 4,281 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41161}{riccardo} & 20,000 & 4,283 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43157}{insurance\_dataset} & 20,000 & 26 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=6}{letter} & 20,000 & 16 & 26 &  &  \\
\href{https://www.kaggle.com/albenft/game-of-thrones-script-all-seasons/Game_of_Thrones_Script.csv}{game-of-thrones-script-all-seasons} & 16,825 & 5 & 43 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44226}{NewspaperChurn} & 15,855 & 16 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=1046}{mozilla4} & 15,545 & 5 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=201}{pol} & 15,000 & 26 & 11 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1471}{eeg-eye-state} & 14,980 & 14 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=44125}{MagicTelescope} & 13,376 & 10 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=26}{nursery} & 12,958 & 8 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=45560}{online-shoppers-intention} & 12,330 & 17 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43395}{Disaster-Tweets} & 11,370 & 4 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=310}{mammography} & 11,183 & 6 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=4534}{PhishingWebsites} & 11,055 & 30 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43622}{Binary-Dataset-of-Phishing-and-Legitimate-URLs} & 11,000 & 14 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=32}{pendigits} & 10,992 & 16 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=46676}{WBCAtt} & 10,298 & 11 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=1459}{artificial-characters} & 10,218 & 7 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=372}{internet\_usage} & 10,108 & 71 & 46 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41165}{robert} & 10,000 & 7,200 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41163}{dilbert} & 10,000 & 2,000 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=45062}{shrutime} & 10,000 & 10 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=375}{JapaneseVowels} & 9,961 & 14 & 9 &  &  \\
\href{https://www.openml.org/search?type=data&id=4538}{GesturePhaseSegmentationProcessed} & 9,873 & 32 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=45554}{FICO-HELOC-cleaned} & 9,871 & 23 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46467}{IBRD\_Loans\_Classification} & 9,215 & 6 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=41972}{Indian\_pines} & 9,144 & 220 & 8 &  &  \\
\href{https://www.openml.org/search?type=data&id=40536}{SpeedDating} & 8,378 & 120 & 2 & \checkmark & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41164}{fabert} & 8,237 & 795 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=24}{mushroom} & 8,124 & 21 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=300}{isolet} & 7,797 & 617 & 26 &  &  \\
\href{https://www.openml.org/search?type=data&id=44157}{eye\_movements} & 7,608 & 23 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1507}{twonorm} & 7,400 & 20 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46280}{blastchar} & 7,043 & 19 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1116}{musk} & 6,598 & 167 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1475}{first-order-theorem-proving} & 6,118 & 51 & 6 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43337}{HMEQ\_Data} & 5,960 & 12 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41145}{philippine} & 5,832 & 308 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=28}{optdigits} & 5,620 & 62 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=4552}{BachChoralHarmony} & 5,586 & 15 & 68 &  &  \\
\href{https://www.openml.org/search?type=data&id=30}{page-blocks} & 5,473 & 10 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=1497}{wall-robot-navigation} & 5,456 & 24 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=41142}{christine} & 5,418 & 1,611 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1489}{phoneme} & 5,404 & 5 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46369}{Is\_fraud} & 5,227 & 19 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41146}{sylvine} & 5,124 & 20 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40900}{Satellite} & 5,100 & 36 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46372}{Multiclass\_Classification\_for\_Corporate\_Credit} & 5,000 & 7 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=43826}{Personal-Loan-Modeling} & 5,000 & 12 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40701}{churn} & 5,000 & 20 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=60}{waveform-5000} & 5,000 & 40 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=46762}{air-quality-and-pollution-assessment} & 5,000 & 9 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=45950}{Heart\_Failure\_Prediction} & 5,000 & 12 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=45039}{compas-two-years} & 4,966 & 11 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40498}{wine-quality-white} & 4,898 & 11 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40983}{wilt} & 4,839 & 5 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44}{spambase} & 4,601 & 57 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43160}{StackOverflow-polarity} & 4,423 & 1 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=1039}{hiva\_agnostic} & 4,229 & 1,617 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46359}{Fraud-Detection-Updated} & 4,156 & 27 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=41156}{ada} & 4,147 & 46 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=504}{analcatdata\_supreme} & 4,052 & 7 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=57}{hypothyroid} & 3,770 & 27 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=4134}{Bioresponse} & 3,751 & 1,776 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40978}{Internet-Advertisements} & 3,279 & 1,558 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40677}{led24} & 3,200 & 24 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=3}{kr-vs-kp} & 3,196 & 36 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46}{splice} & 3,190 & 60 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40670}{dna} & 3,186 & 180 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41158}{gina} & 3,153 & 970 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41144}{madeline} & 3,140 & 259 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41143}{jasmine} & 2,984 & 144 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=23380}{cjs} & 2,796 & 29 & 6 &  &  \\
\href{https://www.openml.org/search?type=data&id=1485}{madelon} & 2,600 & 500 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1487}{ozone-level-8hr} & 2,534 & 72 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40984}{segment} & 2,310 & 16 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1466}{cardiotocography} & 2,126 & 23 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=46597}{Estimation\_of\_Obesity\_Levels} & 2,111 & 16 & 7 &  &  \\
\href{https://www.openml.org/search?type=data&id=1067}{kc1} & 2,109 & 21 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43344}{Corporate-Credit-Rating} & 2,026 & 30 & 8 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=12}{mfeat-factors} & 2,000 & 216 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44227}{South\_Asian\_Churn\_dataset} & 2,000 & 13 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=22}{mfeat-zernike} & 2,000 & 47 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=14}{mfeat-fourier} & 2,000 & 76 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=516}{pbcseq} & 1,945 & 18 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=40982}{steel-plates-fault} & 1,941 & 27 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40975}{car} & 1,728 & 6 & 4 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=40650}{GAMETES\_Heterogeneity} & 1,600 & 20 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1493}{one-hundred-plants-texture} & 1,599 & 64 & 100 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42931}{audit-data} & 1,552 & 35 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1128}{OVA\_Breast} & 1,545 & 10,935 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1457}{amazon-commerce-reviews} & 1,500 & 10,000 & 50 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=181}{yeast} & 1,484 & 8 & 10 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=23}{cmc} & 1,473 & 9 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43893}{ibm-employee-attrition} & 1,470 & 31 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1049}{pc4} & 1,458 & 37 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44230}{Data\_Science\_Nigeria\_Telecoms\_Churn} & 1,400 & 14 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46607}{hepatitis\_c\_virus\_hcv\_for\_egyptian\_patients} & 1,385 & 28 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=43466}{Bank-Note-Authentication-UCI} & 1,372 & 4 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=185}{baseball} & 1,340 & 16 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=40945}{Titanic} & 1,309 & 13 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46719}{mental-health-in-tech-survey} & 1,259 & 26 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=1479}{hill-valley} & 1,212 & 100 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43672}{Heart-Disease-Dataset-(Comprehensive)} & 1,190 & 11 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1542}{volcanoes-e1} & 1,183 & 3 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=43397}{Airlines-Tweets-Sentiments} & 1,097 & 1 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=40966}{MiceProtein} & 1,080 & 77 & 8 &  &  \\
\href{https://www.openml.org/search?type=data&id=1468}{cnae-9} & 1,080 & 856 & 9 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44966}{solar\_flare} & 1,058 & 9 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1494}{qsar-biodeg} & 1,055 & 41 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46709}{SOCC} & 1,043 & 13 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=679}{rmftsa\_sleepdata} & 1,024 & 2 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=1547}{autoUniv-au1-1000} & 1,000 & 20 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40971}{collins} & 1,000 & 19 & 30 &  &  \\
\href{https://www.openml.org/search?type=data&id=31}{credit-g} & 1,000 & 20 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=307}{vowel} & 990 & 12 & 11 &  &  \\
\href{https://www.openml.org/search?type=data&id=43389}{The-Estonia-Disaster-Passenger-List} & 989 & 6 & 2 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=40693}{xd6} & 973 & 9 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40705}{tokyo1} & 959 & 42 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=50}{tic-tac-toe} & 958 & 9 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=45545}{Tour-and-Travels-Customer-Churn-Prediction} & 954 & 6 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=42895}{acp-breast-cancer} & 949 & 1 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=311}{oil\_spill} & 937 & 48 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=2}{anneal} & 898 & 18 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=46592}{Cervical\_Cancer\_Risk\_Factors} & 858 & 30 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=54}{vehicle} & 846 & 18 & 4 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=458}{analcatdata\_authorship} & 841 & 70 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=46604}{glioma\_grading\_clinical\_and\_mutation\_features} & 839 & 23 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=469}{analcatdata\_dmft} & 797 & 4 & 6 &  &  \\
\href{https://www.openml.org/search?type=data&id=46603}{regensburg\_pediatric\_appendicitis} & 780 & 55 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=46585}{QSAR\_Bioconcentration\_classification} & 779 & 12 & 3 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=46254}{Diabetes\_Dataset} & 768 & 8 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1464}{blood-transfusion-service-center} & 748 & 4 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=188}{eucalyptus} & 736 & 19 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=15}{breast-w} & 699 & 9 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40981}{Australian} & 690 & 14 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42}{soybean} & 683 & 35 & 19 &  &  \\
\href{https://www.openml.org/search?type=data&id=470}{profb} & 672 & 8 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46584}{Student\_Performance} & 666 & 11 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=11}{balance-scale} & 625 & 4 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43595}{Loan-Predication} & 614 & 11 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=334}{monks-problems-2} & 601 & 6 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=377}{synthetic\_control} & 600 & 60 & 6 &  &  \\
\href{https://www.openml.org/search?type=data&id=1480}{ilpd} & 583 & 10 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1515}{micro-mass} & 571 & 1,082 & 20 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=1510}{wdbc} & 569 & 30 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=951}{arsenic-male-lung} & 559 & 4 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=6332}{cylinder-bands} & 540 & 34 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=40994}{climate-model-simulation-crashes} & 540 & 18 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=940}{water-treatment} & 527 & 36 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=43643}{Early-Stage-Diabetes-Risk-Prediction-Dataset} & 520 & 16 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=23381}{dresses-sales} & 500 & 12 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=451}{irish} & 500 & 5 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=5}{arrhythmia} & 443 & 262 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=1511}{wholesale-customers} & 440 & 7 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=56}{vote} & 435 & 16 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=455}{cars} & 406 & 7 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=42972}{chronic-kidney-disease} & 400 & 25 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46605}{differentiated\_thyroid\_cancer\_recurrence} & 383 & 16 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=25}{colic} & 368 & 26 & 2 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=13}{breast-cancer} & 286 & 9 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=1495}{qualitative-bankruptcy} & 250 & 6 & 2 &  &  \\
\href{https://www.kaggle.com/imuhammad/us-2020-presidential-election-speeches/us_2020_election_speeches.csv}{us-2020-presidential-election-speeches} & 245 & 5 & 7 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=7}{audiology} & 192 & 57 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46610}{bone\_marrow\_transplant\_children} & 187 & 36 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=46606}{darwin} & 174 & 450 & 2 &  &  \\
\href{https://www.openml.org/search?type=data&id=48}{tae} & 151 & 5 & 3 &  &  \\
\href{https://www.openml.org/search?type=data&id=1099}{EgyptianSkulls} & 150 & 4 & 5 &  &  \\
\href{https://www.openml.org/search?type=data&id=10}{lymph} & 148 & 18 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41157}{arcene} & 100 & 9,920 & 2 & \checkmark &  \\
\end{longtable}



\makeatletter
\setlength\LTcapwidth{\textwidth}
\makeatother
\begin{longtable}{lllll}
\caption{The 93 Regression Datasets of the Pretraining Corpus, with their $n$ examples, $m$ features, presence in a benchmark ($B$) and whether they are textual ($T$).} \label{tab:reg_pretrain_metadata} \\
\toprule
Dataset & $n$ & $m$ & B & T \\
\midrule
\endfirsthead
\caption[]{The 93 Regression Datasets of the Pretraining Corpus.} \\
\toprule
Dataset & $n$ & $m$ & B & T \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{Continued on next page} \\
\midrule
\endfoot
\bottomrule
\endlastfoot
\href{https://www.openml.org/search?type=data&id=40753}{delays\_zurich\_transport} & 5,465,575 & 14 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43573}{New-York-Citi-Bike-Trip} & 4,500,000 & 7 &  &  \\
\href{https://www.openml.org/search?type=data&id=43479}{USA-Airport-Dataset} & 3,606,803 & 14 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43584}{New-York-Taxi-Trip} & 2,083,778 & 21 &  &  \\
\href{https://www.openml.org/search?type=data&id=4549}{Buzzinsocialmedia\_Twitter} & 583,250 & 77 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42729}{nyc-taxi-green-dec-2016} & 581,835 & 18 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43712}{515K-Hotel-Reviews-Data-in-Europe} & 515,738 & 16 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=41167}{dionis} & 416,188 & 54 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42705}{Yolanda} & 400,000 & 100 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42571}{Allstate\_Claims\_Severity} & 188,318 & 130 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42194}{Football\_players\_Fifa\_stats} & 183,142 & 37 &  &  \\
\href{https://www.openml.org/search?type=data&id=41540}{black\_friday} & 166,821 & 9 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44146}{medical\_charges} & 163,065 & 3 & \checkmark &  \\
\href{https://www.kaggle.com/ajinkyablaze/football-manager-data/dataset.csv}{football-manager-data} & 159,541 & 87 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44975}{wave\_energy} & 72,000 & 32 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44974}{video\_transcoding} & 68,784 & 18 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42164}{dating\_profile} & 59,946 & 30 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=42225}{diamonds} & 53,940 & 9 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44976}{sarcos} & 48,933 & 21 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44963}{physiochemical\_protein} & 45,730 & 9 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=564}{fried} & 40,768 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=215}{2dplanes} & 40,768 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=344}{mv} & 40,768 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=43822}{Perth-House-Prices} & 33,656 & 17 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44984}{cps88wages} & 28,155 & 6 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44992}{fps\_benchmark} & 24,624 & 39 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46662}{news\_popularity2} & 24,007 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=574}{house\_16H} & 22,784 & 16 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44993}{health\_insurance} & 22,272 & 11 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42731}{house\_sales} & 21,613 & 21 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44964}{superconductivity} & 21,263 & 81 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44977}{california\_housing} & 20,640 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41210}{avocado-sales} & 18,249 & 11 &  &  \\
\href{https://www.openml.org/search?type=data&id=42712}{Bike\_Sharing\_Demand} & 17,379 & 12 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=216}{elevators} & 16,599 & 18 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43766}{FIFA20-Players} & 14,999 & 72 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44983}{miami\_housing} & 13,932 & 15 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44969}{naval\_propulsion\_plant} & 11,934 & 14 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42688}{Brazilian\_houses} & 10,692 & 11 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43342}{German-House-Prices} & 10,552 & 24 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=44145}{sulfur} & 10,081 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46726}{climate\_change\_impact} & 10,000 & 14 &  &  \\
\href{https://www.openml.org/search?type=data&id=44973}{grid\_stability} & 10,000 & 12 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43618}{Credit-Card-Dataset-for-Clustering} & 8,949 & 16 &  &  \\
\href{https://www.openml.org/search?type=data&id=422}{topo\_2\_1} & 8,885 & 261 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=416}{yprop\_4\_1} & 8,885 & 212 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=46328}{seoul\_bike\_sharing\_demand\_cat} & 8,760 & 13 &  &  \\
\href{https://www.openml.org/search?type=data&id=44981}{pumadyn32nh} & 8,192 & 32 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44980}{kin8nm} & 8,192 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44978}{cpu\_activity} & 8,192 & 21 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=558}{bank32nh} & 8,192 & 32 &  &  \\
\href{https://www.openml.org/search?type=data&id=43648}{Pollen-Luxembourg-1992-2018} & 7,784 & 36 &  &  \\
\href{https://www.openml.org/search?type=data&id=42727}{colleges} & 7,063 & 44 & \checkmark & \checkmark \\
\href{https://www.openml.org/search?type=data&id=503}{wind} & 6,574 & 14 &  &  \\
\href{https://www.openml.org/search?type=data&id=3277}{QSAR-TID-10980} & 5,766 & 1,024 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=3050}{QSAR-TID-11} & 5,742 & 1,024 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43748}{Myanmar-Air-Quality} & 5,122 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=42572}{Santander\_transaction\_value} & 4,459 & 4,735 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=41980}{SAT11-HAND-runtime-regression} & 4,440 & 114 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42570}{Mercedes\_Benz\_Greener\_Manufacturing} & 4,209 & 364 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42726}{abalone} & 4,177 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=529}{pollen} & 3,848 & 4 &  &  \\
\href{https://www.openml.org/search?type=data&id=507}{space\_ga} & 3,107 & 6 & \checkmark &  \\
\href{https://www.kaggle.com/neilcosgrove/scotch-whiskey-reviews-update-2020/scotch_review2020.csv}{scotch-whiskey-reviews-update-2020} & 2,247 & 4 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=550}{quake} & 2,178 & 3 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44958}{auction\_verification} & 2,043 & 7 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42730}{us\_crime} & 1,994 & 126 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44957}{airfoil\_self\_noise} & 1,503 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=42165}{house\_prices} & 1,460 & 80 &  &  \\
\href{https://www.openml.org/search?type=data&id=42563}{house\_prices\_nominal} & 1,460 & 79 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43653}{NBA-PLAYERS--2016-2019} & 1,408 & 43 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=43463}{Insurance-Premium-Data} & 1,338 & 6 &  &  \\
\href{https://www.openml.org/search?type=data&id=41021}{Moneyball} & 1,232 & 14 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=541}{socmob} & 1,156 & 5 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43071}{MIP-2016-regression} & 1,090 & 116 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44965}{geographical\_origin\_of\_music} & 1,059 & 116 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44959}{concrete\_compressive\_strength} & 1,030 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=43588}{Household-monthly-electricity-bill} & 1,000 & 9 &  &  \\
\href{https://www.openml.org/search?type=data&id=223}{stock} & 950 & 9 &  &  \\
\href{https://www.openml.org/search?type=data&id=44970}{QSAR\_fish\_toxicity} & 908 & 6 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44994}{cars} & 804 & 17 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=44960}{energy\_efficiency} & 768 & 8 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=563}{kdd\_el\_nino-small} & 709 & 8 &  &  \\
\href{https://www.openml.org/search?type=data&id=44967}{student\_performance\_por} & 649 & 30 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=549}{strikes} & 625 & 6 &  &  \\
\href{https://www.openml.org/search?type=data&id=546}{sensory} & 576 & 11 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=566}{meta} & 528 & 21 &  &  \\
\href{https://www.openml.org/search?type=data&id=44962}{forest\_fires} & 517 & 12 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=666}{rmftsa\_ladata} & 508 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=531}{boston} & 506 & 13 & \checkmark &  \\
\href{https://www.openml.org/search?type=data&id=547}{no2} & 500 & 7 &  &  \\
\href{https://www.openml.org/search?type=data&id=44223}{Diabetes(scikit-learn)} & 442 & 10 &  &  \\
\href{https://www.openml.org/search?type=data&id=43420}{NBA-2k20-player-dataset} & 439 & 14 &  & \checkmark \\
\href{https://www.openml.org/search?type=data&id=525}{baseball-hitter} & 263 & 22 &  &  \\
\href{https://www.openml.org/search?type=data&id=560}{bodyfat} & 252 & 14 &  &  \\
\href{https://www.openml.org/search?type=data&id=43660}{Lisbon-House-Prices} & 246 & 13 &  &  \\
\href{https://www.openml.org/search?type=data&id=505}{tecator} & 240 & 124 & \checkmark &  \\
\end{longtable}


\section{Benchmark Datasets}
\label{app:benchmark}

This appendix elaborates on the benchmark presented in \S\appref{sec:exp:benchmark}. We consider all datasets proposed by \textit{AutoML Multimodal Benchmark} (SHI) \cite{shi_benchmarking_2021}, \textit{Vectorizing} (VEC) \cite{grinsztajn_vectorizing_2023}, and \textit{CARTE-Benchmark} (CRT) \cite{kim_carte_2024}, resulting in a final set of 50 datasets. We deduplicate datasets that appear as-is in more than one benchmark. In addition, since CARTE explores the concept of multi-table learning, they introduce highly-overlapping datasets for which we remove one variant (see 4.3 and B.2 in their paper).

Table~\ref{tab:cls_benchmarks_metadata} presents the classification datasets and Table~\ref{tab:reg_benchmarks_metadata} the regression ones. Each table includes an internal \textit{ID} used for reference, the \textit{Dataset} name, the number of examples $n$ and of features $m$, and the number of classes $C$ for classification.\footnote{We treat ranking problems with up to 10 discrete values as multiclass problems.} Finally, we also indicate the benchmark sources where each dataset appears. In addition, Table~\ref{tab:benchmarks_description} presents the full benchmark with a short description per dataset, and Table~\ref{tab:benchmarks_exclusion} details the datasets removed during the deduplication process. Most of the excluded datasets are regression datasets from the \textit{CARTE-Benchmark}, because of its high-overlapping nature. 



\begin{table}[!htbp]
\caption{The 14 classification datasets of the benchmark, with their $n$ examples, $m$ features, $C$ classes, and presence in the SHI, VEC and CRT benchmarks.}
\label{tab:cls_benchmarks_metadata}
\begin{tabular}{llllllll}
\toprule
ID & Dataset & $n$ & $m$ & $C$ & SHI & VEC & CRT \\
\midrule
C01 & \href{https://www.openml.org/search?type=data&id=46659}{women\_clothing\_review} & 18,788 & 10 & 5 & \checkmark &  &  \\
C02 & \href{https://www.kaggle.com/sobhanmoosavi/us-accidents/US_Accidents_March23.csv}{us-accidents} & 7,728,394 & 42 & 4 &  & \checkmark & \checkmark \\
C03 & \href{https://www.openml.org/search?type=data&id=46664}{data\_scientist\_salary} & 15,841 & 6 & 6 & \checkmark &  &  \\
C04 & \href{https://www.openml.org/search?type=data&id=46667}{imdb\_genre\_prediction} & 800 & 11 & 2 & \checkmark &  &  \\
C05 & \href{https://www.openml.org/search?type=data&id=46651}{product\_sentiment\_machine\_hack} & 5,091 & 2 & 4 & \checkmark &  &  \\
C06 & \href{https://www.openml.org/search?type=data&id=46658}{google\_qa\_question\_type\_reason} & 4,863 & 39 & 5 & \checkmark &  &  \\
C07 & \href{https://www.kaggle.com/ngshiheng/michelin-guide-restaurants-2021/michelin_my_maps.csv}{michelin-guide-restaurants-2021} & 17,735 & 11 & 5 &  &  & \checkmark \\
C08 & \href{https://www.openml.org/search?type=data&id=46655}{fake\_job\_postings2} & 12,725 & 5 & 2 & \checkmark &  &  \\
C09 & \href{https://www.openml.org/search?type=data&id=46654}{jigsaw\_unintended\_bias100K} & 100,000 & 40 & 2 & \checkmark &  &  \\
C10 & \href{https://www.kaggle.com/omkarsabnis/yelp-reviews-dataset/yelp.csv}{yelp-reviews-dataset} & 10,000 & 5 & 5 &  &  & \checkmark \\
C11 & \href{https://www.openml.org/search?type=data&id=46652}{news\_channel} & 20,284 & 17 & 6 & \checkmark &  &  \\
C12 & \href{https://www.openml.org/search?type=data&id=46653}{wine\_reviews} & 84,123 & 5 & 30 & \checkmark & \checkmark & \checkmark \\
C13 & \href{https://www.openml.org/search?type=data&id=46668}{kick\_starter\_funding} & 86,502 & 9 & 2 & \checkmark &  &  \\
C14 & \href{https://www.openml.org/search?type=data&id=46665}{melbourne\_airbnb} & 18,316 & 89 & 10 & \checkmark &  &  \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}[!htbp]
\caption{The 36 regression datasets of the benchmark, with their $n$ examples, $m$ features, and presence in the SHI, VEC and CRT benchmarks.}
\label{tab:reg_benchmarks_metadata}
\begin{tabular}{lllllll}
\toprule
ID & Dataset & $n$ & $m$ & SHI & VEC & CRT \\
\midrule
R01 & \href{https://www.kaggle.com/sukritchatterjee/used-cars-dataset-cardekho/cars_details_merges.csv}{used-cars-dataset-cardekho} & 37,814 & 112 &  &  & \checkmark \\
R02 & \href{https://www.kaggle.com/bogdansorin/second-hand-mercedes-benz-registered-2000-2023-ita/mercedes-benz.csv}{second-hand-mercedes-benz} & 16,392 & 7 &  &  & \checkmark \\
R03 & \href{https://www.kaggle.com/hernan4444/animeplanet-recommendation-database-2020/anime.csv}{animeplanet-recommendation} & 14,391 & 14 &  &  & \checkmark \\
R04 & \href{https://ai-jobs.net/salaries/download/salaries.csv}{ML/DS-Salaries} & 119,628 & 9 &  &  & \checkmark \\
R05 & \href{http://pages.cs.wisc.edu/~anhai/data/784_data/baby_products/csv_files/babies_r_us.csv}{Babies-R-Us} & 5,085 & 12 &  &  & \checkmark \\
R06 & \href{https://www.openml.org/search?type=data&id=42125}{employee\_salaries} & 9,228 & 11 &  & \checkmark & \checkmark \\
R07 & \href{https://www.kaggle.com/maharshipandya/-spotify-tracks-dataset/dataset.csv}{spotify-tracks-dataset} & 114,000 & 18 &  & \checkmark &  \\
R08 & \href{https://www.openml.org/search?type=data&id=46669}{california\_house\_price} & 37,951 & 39 & \checkmark &  &  \\
R09 & \href{https://www.openml.org/search?type=data&id=45012}{fifa} & 19,178 & 28 &  &  & \checkmark \\
R10 & \href{https://www.kaggle.com/hanifalirsyad/coffee-scrap-coffeereview/coffee_clean.csv}{coffee-scrap-coffeereview} & 2,440 & 17 &  &  & \checkmark \\
R11 & \href{http://pages.cs.wisc.edu/~anhai/data/784_data/bikes/csv_files/bikewale.csv}{BikeWale} & 9,003 & 6 &  & \checkmark & \checkmark \\
R12 & \href{https://www.kaggle.com/mustafaimam/used-car-prices-in-pakistan-2021/Used_car_prices_in_Pakistan_cleaned.csv}{used-car-prices-in-pakistan} & 72,655 & 9 &  &  & \checkmark \\
R13 & \href{https://www.openml.org/search?type=data&id=46663}{bookprice\_prediction} & 4,989 & 8 & \checkmark &  &  \\
R14 & \href{https://www.openml.org/search?type=data&id=46656}{ae\_price\_prediction} & 22,662 & 12 & \checkmark &  &  \\
R15 & \href{https://opendata.vancouver.ca/api/records/1.0/download/?dataset=employee-remuneration-and-expenses-earning-over-75000&format=csv}{Employee-remuneration} & 44,574 & 5 &  & \checkmark & \checkmark \\
R16 & \href{https://www.kaggle.com/stefanoleone992/filmtv-movies-dataset/filmtv_movies.csv}{filmtv-movies-dataset} & 41,399 & 17 &  &  & \checkmark \\
R17 & \href{https://www.kaggle.com/peopledatalabssf/free-7-million-company-dataset/companies_sorted.csv}{free-7-million-company-dataset} & 7,173,426 & 7 &  & \checkmark & \checkmark \\
R18 & \href{https://www.kaggle.com/markusschmitz/museums/museums_prep.csv}{museums} & 22,290 & 21 &  &  & \checkmark \\
R19 & \href{https://www.kaggle.com/joshuakalobbowles/vivino-wine-data/vivino.csv}{vivino-wine-data} & 8,650 & 6 &  &  & \checkmark \\
R20 & \href{https://www.kaggle.com/limtis/wikiliq-dataset/spirits_data.csv}{wikiliq-dataset} & 12,569 & 12 &  &  & \checkmark \\
R21 & \href{https://www.kaggle.com/ruthgn/beer-profile-and-ratings-data-set/beer_profile_and_ratings.csv}{beer-profile-and-ratings} & 3,197 & 24 &  &  & \checkmark \\
R22 & \href{https://www.kaggle.com/noorrizki/top-korean-drama-list-1500/kdrama_list.csv}{korean-drama} & 1,647 & 9 &  &  & \checkmark \\
R23 & \href{https://www.kaggle.com/gregorut/videogamesales/vgsales.csv}{videogamesales} & 16,598 & 5 &  &  & \checkmark \\
R24 & \href{https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants/zomato.csv}{zomato-bangalore-restaurants} & 41,665 & 15 &  & \checkmark & \checkmark \\
R25 & \href{https://www.kaggle.com/rounakbanik/the-movies-dataset/movies_metadata.csv}{the-movies-dataset} & 45,460 & 20 &  &  & \checkmark \\
R26 & \href{https://www.kaggle.com/mattop/nba-draft-basketball-player-data-19892021/nbaplayersdraft.csv}{nba-draft-basketball} & 1,669 & 22 &  &  & \checkmark \\
R27 & \href{http://pages.cs.wisc.edu/~anhai/data/784_data/books2/csv_files/goodreads.csv}{Goodreads} & 3,967 & 14 &  & \checkmark &  \\
R28 & \href{http://pages.cs.wisc.edu/~anhai/data/784_data/movies1/csv_files/rotten_tomatoes.csv}{Rotten-Tomatoes} & 7,158 & 15 &  &  & \checkmark \\
R29 & \href{https://www.kaggle.com/turkibintalib/saudi-arabia-used-cars-dataset/UsedCarsSA_Clean_EN.csv}{saudi-arabia-used-cars-dataset} & 8,035 & 12 &  &  & \checkmark \\
R30 & \href{https://www.kaggle.com/ankanhore545/top-ramen-ratings-2022/Top Ramen Ratings .csv}{top-ramen-ratings-2022} & 4,105 & 4 &  & \checkmark & \checkmark \\
R31 & \href{https://www.scimagojr.com/journalrank.php?out=xls}{Journal-Score-SJR} & 31,136 & 21 &  &  & \checkmark \\
R32 & \href{https://www.kaggle.com/rtatman/chocolate-bar-ratings/flavors_of_cacao.csv}{chocolate-bar-ratings} & 1,795 & 8 &  &  & \checkmark \\
R33 & \href{https://www.openml.org/search?type=data&id=46660}{mercari\_price\_suggestion100K} & 100,000 & 9 & \checkmark &  &  \\
R34 & \href{https://www.kaggle.com/skamlo/wine-price-on-polish-market/wine.csv}{wine-price-on-polish-market} & 2,247 & 18 &  &  & \checkmark \\
R35 & \href{https://www.kaggle.com/verracodeguacas/clear-corpus/CLEAR.csv}{clear-corpus} & 4,724 & 30 &  & \checkmark & \checkmark \\
R36 & \href{https://www.openml.org/search?type=data&id=46661}{jc\_penney\_products} & 10,860 & 5 & \checkmark &  &  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{Benchmark Datasets Description}
\label{tab:benchmarks_description}
\begin{tabular}{ll}
\toprule
ID & Description \\
\midrule
C01 & Women Clothing E-Commerce Reviews \\
C02 & US Accidents between 2016 and 2023 \\
C03 & Indian Data Scientist Salary Prediction \\
C04 & IMDB Movies Genre Prediction \\
C05 & Product Sentiment Analysis \\
C06 & Google QA Question Type Reason Explanation \\
C07 & Michelin Guide Restaurants Awards \\
C08 & Fake Job Posting Detection \\
C09 & Online Social Media Comments Toxicity \\
C10 & YELP Dataset Reviews \\
C11 & News Channel Prediction \\
C12 & Wine Reviews for Variety Prediction \\
C13 & Kickstarter Funding Prediction \\
C14 & Melbourne AirBnB Listings \\
R01 & User cars and listing price in the website Cardekho \\
R02 & Second-hand cars Mercedes Benz price Italy \\
R03 & Anime-Planet Recommendation Database 2020 \\
R04 & Salaries of ML/DS Professionals Worldwide \\
R05 & Prices Prediction for baby product from Babies R Us website \\
R06 & Employee Salary in Montgomery County, MD \\
R07 & Spotify Tracks Popularity \\
R08 & California Houses 2020 Prices \\
R09 & FIFA 2022 Players Wages \\
R10 & Coffee Review Rating \\
R11 & Bike and scooters from bikewale website in India \\
R12 & Used car prices in Pakistan 2021 \\
R13 & Book Price Prediction \\
R14 & American Eagle Retailer Price Prediction \\
R15 & Employee Remuneration and Expenses - Vancouver \\
R16 & FilmTV movies ataset rating \\
R17 & Company size prediction \\
R18 & General information on the US museums \\
R19 & Vivino Spanish Wine Data \\
R20 & WikiliQ - Alcohol dataset (May, 2022) \\
R21 & Tasting profiles and consumer reviews for beers \\
R22 & Korean Dramas \\
R23 & Video Games Sales \\
R24 & Zomato Restaurants in Bengaluru \\
R25 & Metadata of movies released until 2017 for box-office revenues \\
R26 & NBA Draft Basketball Player Data 1989-2021 \\
R27 & Books ratings \\
R28 & Rotten Tomatoes Movie Ratings \\
R29 & Saudi Arabia Used Cars Price from Syarah Website \\
R30 & Ramen Ratings \\
R31 & Academic impact for Scientific Journals \\
R32 & Chocolate Bar expert ratings \\
R33 & Mercari Online Marketplace Product Prices \\
R34 & Information about wines on the polish market \\
R35 & Readability scores for text passages spanning various genres and time periods \\
R36 & JC Penney Product Prices in Retailer Website \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Excluded datasets, with their benchmark origin and reason for removal: (1) \textit{Duplicate} dataset, (2) \textit{Unavailable}, for datasets with inconvenient or unavailable hosting outside tabular repositories, and (3) \textit{Pretraining}, for two (regression) datasets mistakenly used for the pretraining.}
\label{tab:benchmarks_exclusion}
\begin{tabular}{llll}
\toprule
\textbf{Dataset} & \textbf{Benchmark} & \textbf{Reason} & \textbf{Duplicate} \\
\midrule
google\_qa\_answer\_type   & SHI   & Duplicate & google\_qa\_question\_type \\
news\_popularity2   & SHI   & Pretraining \\
US Presidential & VEC & Unavailable \\
Journal Influence & VEC & Duplicate & Journal-Score-SJR \\
Buy Buy Baby & CRT & Duplicate & Babies-R-Us \\
Bikedekho & CRT & Duplicate & BikeWale \\
Journal Score JCR & CRT & Duplicate & Journal-Score-SJR \\
Japanese Anime & CRT & Duplicate & animeplanet-recommendation \\
Mydramalist & CRT & Duplicate & korean-drama \\
Prescription Drugs & CRT & Unavailable \\
Roger Ebert & CRT & Unavailable \\
US Presidential & CRT & Unavailable \\
Used Cars 24 & CRT & Duplicate & used-car-prices-in-pakistan \\
Whisky & CRT & Pretraining \\
Wine.com & CRT & Duplicate & wine\_reviews \\
WineEnthusiasts & CRT & Duplicate & wine\_reviews \\
\bottomrule
\end{tabular}
\end{table}


\section{Baselines}
\label{app:baselines}

In this appendix we first discuss models excluded from the evaluation due to data leakage concerns, and then cover implementation details for the baselines used in our main experiments \S\appref{sec:exp:baselines}.

\subsection{Excluded Baselines}
\label{app:baseline_exclude}


As opposed to GDBTs or single-dataset deep learning methods, evaluating pretrained tabular models introduces additional complexity. Indeed, leakage can come in multiple forms. When LLMs are involved, there is a risk of memorization \cite{bordt_elephants_2024}, and models trained on synthetic datasets \cite{hollmann_accurate_2025} which try to mimic real-world distributions, can be unintentionally biased towards popular benchmarks. 

While these two forms of leakage are subtle and hard to detect, a more direct form must be strictly avoided: When the same dataset (or a variant of it) is used during pretraining, and then it is evaluated as a downstream task. In such scenario there is inevitable severe data leakage, especially when running with multiple random test splits. The rest of the section explains how both \textit{TP-BERTa} \cite{yan_making_2023} and \textit{CM2} \cite{ye_towards_2024} suffer from such contamination with respect to our benchmark. As we briefly mention in \S\appref{sec:discussion}, we advocate for improving TFM research by encouraging models that are practical to evaluate, by releasing several versions of each model, and providing default hyperparameters.

\paragraph{TP-BERTa} We exclude TP-BERTa from our evaluation for two key reasons. First, their implementation assumes that every example is treated as a serialized single sequence, which allows for a maximum length of 512 tokens, as elaborated in Appendix~\ref{app:arch:verbalize_semantic}. While this decision is efficient for datasets with a low amount of features and no free-text presence, around half of the datasets in our benchmark are too long for that limitation, as they either contain too many features or long free-texts. Second, TP-BERTa's pretraining uses datasets that appear in our evaluation set, as listed in Table 6 of their paper \cite{yan_making_2023}. It is evident that several datasets overlap directly with datasets in our benchmark \S\appref{sec:exp:benchmark} (e.g., \textit{1510\_fifa}, \textit{1368\_IMDb-Ratings}, \textit{1639\_Melbourne}), disqualifying them for our purposes. Furthermore, we observe a concerning overlap between their pretraining and downstream task datasets (e.g., \textit{airlines}, \textit{sf police} and \textit{diabetes}). We believe that this questions the validity of their evaluation, and that such contamination poses a serious challenge for the TFM community which could be substantially addressed by better tabular data repositories \cite{tschalzev_unreflected_2025}.

\paragraph{CM2} CM2 was pretrained over \textit{OpenTabs}, a compilation of more than 2,000 datasets drawn from public tabular data repositories, including OpenML and Kaggle. While this collection is valuable, pretraining a model over these datasets compromises further evaluation of any of them. Naturally, the overlap with our benchmark here is extremely high, making it infeasible to use as a baseline. Interestingly, their repository\footnote{https://github.com/Chao-Ye/CM2} lists TP-BERTa as a method trained on a subset of OpenTabs, reinforcing that the leakage is shared between the models.

\subsection{Baselines Implementation and Hyperparameters} \label{app:baselines_used} 

This section outlines the implementation and hyperparameter tuning strategy used for the baselines reported in \S\appref{sec:exp:baselines}. While each baseline has its own model-specific preprocessing pipeline, we apply two shared preprocessing steps to both TabSTAR (as detailed in Appendix~\ref{app:arch:verbalization}) and all the baselines: (1) We perform date preprocessing by using skrub's \textit{DatetimeEncoder}, and (2) Apply a clipped z-score transformation for target variables in regression datasets.

\paragraph{Textual Feature Handling}\label{app:baselines_text_feat} CARTE natively supports textual inputs, and the TabPFN-v2 API client\footnote{https://github.com/PriorLabs/tabpfn-client} does as well, although its implementation details remain undisclosed. On the other hand, GBDTs do not natively support free-text features,\footnote{CatBoost includes a built-in text module, but it underperforms compared to dense text embeddings.}, so we preprocess these features into fixed-size embeddings using \textit{skrub}'s \textit{TextEncoder}, which internally applies a frozen \textit{e5-small-v2} encoder to each semantic column. This aligns with the encoder used in TabSTAR, enabling a fair comparison across models. There are, however, two key differences in how TabSTAR handles these embeddings: First, the embeddings are specifically finetuned for the task, contributing significantly to its strong performance as shown in \S\appref{sec:results} and further analyzed in \S\appref{app:analysis_unfreeze}. The second detail is that skrub applies dimensionality reduction to 30 dimensions, as proposed by \cite{grinsztajn_vectorizing_2023}. This compressed representation performs comparably to the full embedding space, while offering improved inference efficiency.


\subsubsection{TabPFN-v2}

We run TabPFN-v2 using their API client which supports text features.\footnote{We use v2.0.8, the latest version available at the time of running the experiments.} While the intrinsic details of their textual handling remain undocumented, it's reasonable to assume that it resembles the processing we apply to GBDTs, as their model leverages ICL and their architecture has no textual encoder.

\subsubsection{CARTE}

We run CARTE using its package,\footnote{https://github.com/soda-inria/carte} which inherently performs $k$-fold cross-validation. After consulting with the authors, we set $k=5$ for efficiency instead of their default, 10. We do grid search over their recommended learning rates,\footnote{$\{2.5\times10^{-4},\;5\times10^{-4},\;7.5\times10^{-4},\;2.5\times10^{-3},\;5\times10^{-3},\;7.5\times10^{-3}\}$
} and we take the best-performing variant per dataset split.

\subsubsection{CatBoost and CatBoost-Tuned}

We run CatBoost using the \textit{catboost} package\footnote{https://pypi.org/project/catboost/} and run the default configuration suggested by \cite{gorishniy_revisiting_2021} by setting $early\_stopping\_rounds=50$, $od\_pval=0.001$, $iterations = 2000$. For the tuned version, we use the \textit{Optuna} package\footnote{https://pypi.org/project/optuna/} with random search, with a budget of 4 hours for every run and parallelizing trials on 8 CPU cores. We use 5-fold cross-validation and take the best configuration selected based on this mean score. We use it then to retrain the model on the full training data. For hyperparameter space, we follow the hyperparameter search suggested by \cite{hollmann_accurate_2025} as detailed in Table~\ref{tab:cat_hyp}.


\begin{table}[h]
\centering
\caption{CatBoost-Tuned Hyperparameters Search Space}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
lr & $\log \mathcal{U}(e^{-5}, 1)$ \\
random\_strength & $\mathcal{U}\{1, 2, \ldots, 20\}$ \\
l2\_leaf\_reg & $\log \mathcal{U}(1, 10)$ \\
bagging\_temperature & $\mathcal{U}(0.0, 1.0)$ \\
leaf\_estimation\_iterations & $\mathcal{U}\{1, 2, \ldots, 20\}$ \\
iterations & $\mathcal{U}\{100, 101, \ldots, 4000\}$ \\
\bottomrule
\end{tabular}
\label{tab:cat_hyp}
\end{table}

\subsubsection{XGBoost and XGBoost-Tuned}
\label{app:xgboost}

We run XGBoost using the \textit{xgboost} package\footnote{https://pypi.org/project/xgboost/} and follow the same procedure as for CatBoost, except additional preprocessing  (e.g., transforming categorical variables into one-hot encodings). For the default configuration, we follow the suggestion of \cite{gorishniy_revisiting_2021} and use: $booster="gbtree"$, $early\_stopping\_rounds=50$, $n\_estimators=2000$. For the tuned variant, we follow the hyperparameter search space suggested by \cite{hollmann_accurate_2025}, as shown in Table~\ref{tab:xgb_hyp}.

\begin{table}[h]
\centering
\caption{XGBoost-Tuned Hyperparameters Search Space}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
learning\_rate & $\log \mathcal{U}(e^{-7}, 1)$ \\
max\_depth & $\mathcal{U}\{1, 2, \ldots, 10\}$ \\
subsample & $\mathcal{U}(0.2, 1)$ \\
colsample\_bytree & $\mathcal{U}(0.2, 1)$ \\
colsample\_bylevel & $\mathcal{U}(0.2, 1)$ \\
min\_child\_weight & $\log \mathcal{U}(e^{-16}, e^{5})$ \\
alpha & $\log \mathcal{U}(e^{-16}, e^{2})$ \\
reg\_lambda & $\log \mathcal{U}(e^{-16}, e^{2})$ \\
gamma & $\log \mathcal{U}(e^{-16}, e^{2})$ \\
n\_estimators & $\mathcal{U}\{100, 101, \ldots, 4000\}$ \\
\bottomrule
\end{tabular}
\label{tab:xgb_hyp}
\end{table}


\subsubsection{Random Forest}

We treat Random Forest as a weak baseline to establish a lower-bound reference for each dataset split. We run it with the sklearn package \footnote{https://scikit-learn.org/} and use its default configuration with $n\_estimators=100$.



\section{Extended Main Results}
\label{app:results}

In this appendix we provide the main results for the experiment as reported in \S\appref{sec:results}. As elaborated in \S\appref{sec:main_experiments}, each model is evaluated on each dataset across 10 splits. Since performance scales vary between datasets, we follow the normalization approach proposed by \cite{hollmann_accurate_2025}, rescaling all scores to the $[0, 1]$ range. The final reported performance for each model is the average over these normalized runs, and we compute 95\% confidence intervals using the standard normal approximation: $\displaystyle \hat{\mu} \pm 1.96\,\frac{\hat{\sigma}}{\sqrt{n}}$. 

In this section, we often use abbreviated model names for conciseness: We refer to CatBoost variants as \textit{CatB} and \textit{CatB-T}, with the latter being the Tuned version. Similarly, we use \textit{XGB} and \textit{XGB-T} for XGBoost. We maintain the abbreviation of Random Forest (RF) and shorten TabPFN-v2 to simply \textit{TabPFN}. For models that run on an unlimited number of training examples, we add a \textit{-U} suffix, i.e. \textit{TabSTAR-U}, \textit{CatB-T-U}, and \textit{XGB-T-U}.


\subsection{Dataset Level Performance}
\label{app:res:dataset_lavel}

We report AUROC for classification and $R^2$ for regression, with 95\% CIs computed over the 10 runs for each dataset. Tables ~\ref{tab:cls_10k_dataset_performance} and \ref{tab:cls_unlimit_dataset_performance} summarize classification performance on datasets with up to 10K and over 10K examples, respectively. Tables~\ref{tab:reg_10k_dataset_performance} and~\ref{tab:reg_unlimit_dataset_performance} to the same for regression tasks. For conciseness, datasets are referred by their ID from Appendix~\ref{app:benchmark}.

As discussed in \S\appref{sec:results}, TabPFN-v2 is unable to run over 4 datasets: C12, because it is a multiclass problem with more than 10 classes, and C02, C14 and R01 because they support inference for up to 500,000 cells. Attempts to run the model over a subset of the examples led to a significantly worse performance, and thus we decide not to report them to allow a fair comparison. Furthermore, CARTE is unable to run over 15 of the datasets in the benchmark due a known bug\footnote{https://github.com/soda-inria/carte/issues/23} in their implementation for the \textit{PowerTransformation}, which struggles in the presence of features with too less unique values.

\begin{table}
\center
\footnotesize
\caption{Classification performance per dataset (up to 10K). The top performance score is bolded first, and then all scores are rounded. We report average AUROC with 95\% CIs.}
\label{tab:cls_10k_dataset_performance}
\begin{tabular}{lllllllll}
\toprule
ID & CARTE & CatB & CatB-T & RF & TabPFN & TabSTAR & XGB & XGB-T \\
\midrule
C01 & 88.4±0.3 & 90.2±0.3 & 90.3±0.4 & 88.8±0.3 & 90.3±0.3 & \textbf{90.8±0.3} & 89.3±0.4 & 90.2±0.4 \\
C02 &  & 97.3±0.5 & 97.4±0.4 & 96.3±0.5 &  & \textbf{97.9±0.5} & 97.2±0.3 & 97.6±0.3 \\
C03 & 82.5±0.3 & 82.0±0.3 & 81.2±0.6 & 77.3±0.3 & 82.4±0.3 & \textbf{83.0±0.3} & 80.5±0.3 & 82.1±0.3 \\
C04 &  & 84.5±1.9 & 85.4±1.8 & 82.4±3.0 & \textbf{88.3±1.5} & 83.7±2.3 & 80.7±2.9 & 85.4±2.0 \\
C05 & 88.5±0.9 & 90.6±1.0 & 90.9±0.5 & 88.0±1.1 & 91.2±0.7 & \textbf{91.3±0.8} & 88.1±1.1 & 90.3±0.6 \\
C06 & 80.9±1.5 & 81.3±1.3 & 82.6±1.1 & 73.6±1.2 & \textbf{87.7±0.5} & 87.0±0.6 & 81.5±1.0 & 83.7±0.9 \\
C07 & 90.1±0.4 & 90.3±0.3 & 90.6±0.3 & 85.1±0.7 & 89.8±0.4 & \textbf{91.5±0.3} & 87.2±0.6 & 89.3±0.5 \\
C08 & 90.8±1.6 & 93.0±0.9 & 93.2±1.0 & 90.2±1.3 & 91.3±1.1 & 93.5±1.5 & 91.9±1.1 & \textbf{94.4±0.7} \\
C09 &  & 99.8±0.1 & 99.8±0.1 & 99.6±0.2 & \textbf{99.8±0.1} & 99.3±0.1 & 99.7±0.1 & 99.8±0.1 \\
C10 &  & 86.7±0.2 & 87.0±0.2 & 84.6±0.2 & 87.6±0.4 & \textbf{89.1±0.2} & 85.4±0.3 & 86.8±0.2 \\
C11 & 78.4±0.5 & 79.7±0.5 & 80.7±0.4 & 76.2±0.6 & \textbf{81.4±0.4} & 79.2±0.5 & 78.2±0.6 & 80.2±0.5 \\
C12 & 96.0±0.2 & 97.5±0.1 & 97.7±0.1 & 95.3±0.2 &  & \textbf{98.3±0.1} & 96.6±0.2 & 97.3±0.1 \\
C13 & 70.9±0.8 & 73.7±1.1 & 74.1±1.0 & 71.1±1.1 & 72.3±0.9 & \textbf{75.0±0.7} & 70.2±0.8 & 74.1±1.1 \\
C14 &  & 83.5±0.3 & 84.0±0.5 & 80.1±0.3 &  & \textbf{84.0±0.3} & 81.4±0.4 & 83.2±0.4 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\center
\footnotesize
\caption{Classification performance per dataset (above 10K). The top performance score is bolded first, and then all scores are rounded. We report average AUROC with 95\% CIs.}
\label{tab:cls_unlimit_dataset_performance}
\begin{tabular}{llllllll}
\toprule
ID & CatB-T & CatB-T-U & TabPFN & TabSTAR & TabSTAR-U & XGB-T & XGB-T-U \\
\midrule
C01 & 90.3±0.4 & 90.5±0.4 & 90.3±0.3 & 90.8±0.3 & \textbf{91.2±0.3} & 90.2±0.4 & 90.4±0.3 \\
C02 & 97.4±0.4 & 98.3±0.3 &  & 97.9±0.5 & \textbf{98.4±0.2} & 97.6±0.3 & 98.2±0.3 \\
C03 & 81.2±0.6 & 81.5±0.8 & 82.4±0.3 & 83.0±0.3 & \textbf{83.8±0.5} & 82.1±0.3 & 82.3±0.3 \\
C07 & 90.6±0.3 & 91.0±0.4 & 89.8±0.4 & 91.5±0.3 & \textbf{91.9±0.3} & 89.3±0.5 & 89.9±0.5 \\
C08 & 93.2±1.0 & 93.1±1.2 & 91.3±1.1 & 93.5±1.5 & \textbf{95.1±0.9} & 94.4±0.7 & 94.4±0.9 \\
C09 & 99.8±0.1 & \textbf{99.8±0.1} & 99.8±0.1 & 99.3±0.1 & 99.5±0.1 & 99.8±0.1 & 99.8±0.1 \\
C11 & 80.7±0.4 & \textbf{81.8±0.4} & 81.4±0.4 & 79.2±0.5 & 81.0±0.5 & 80.2±0.5 & 81.4±0.4 \\
C12 & 97.7±0.1 & 98.5±0.1 &  & 98.3±0.1 & \textbf{99.1±0.1} & 97.3±0.1 & 98.4±0.1 \\
C13 & 74.1±1.0 & 76.7±0.8 & 72.3±0.9 & 75.0±0.7 & \textbf{77.9±1.0} & 74.1±1.1 & 76.9±0.9 \\
C14 & 84.0±0.5 & 84.8±0.4 &  & 84.0±0.3 & \textbf{85.1±0.3} & 83.2±0.4 & 84.2±0.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\center
\footnotesize
\caption{Regression performance per dataset. The top performance score is bolded first, and then all scores are rounded. We report average  $R^2$ with 95\% CIs.}
\label{tab:reg_10k_dataset_performance}
\begin{tabular}{lllllllll}
\toprule
ID & CARTE & CatB & CatB-T & RF & TabPFN & TabSTAR & XGB & XGB-T \\
\midrule
R01 & 100±0.0 & 100±0.0 & 100±0.0 & \textbf{100±0.0} &  & 100±0.0 & 99.9±0.1 & 100±0.0 \\
R02 &  & 98.3±1.0 & 98.3±0.9 & 98.0±1.0 & \textbf{98.4±0.9} & 98.0±1.1 & 97.9±1.1 & 98.3±0.9 \\
R03 & 71.8±0.5 & 73.8±0.4 & 74.1±0.3 & 68.4±0.6 & \textbf{74.9±0.4} & 70.9±0.8 & 69.1±0.5 & 73.7±0.5 \\
R04 &  & 86.2±7.4 & 85.1±7.1 & 85.1±6.8 & 86.1±5.2 & 81.5±8.7 & 85.8±7.5 & \textbf{87.5±3.7} \\
R05 & \textbf{93.2±0.8} & 89.5±1.3 & 90.0±1.2 & 86.7±1.3 & 92.7±0.9 & 93.2±1.2 & 87.1±1.4 & 90.1±1.3 \\
R06 & 97.7±0.8 & 98.1±0.5 & 98.2±0.5 & 97.2±1.0 & \textbf{98.5±0.6} & 98.1±0.3 & 97.1±0.7 & 97.9±0.7 \\
R07 & \textbf{71.6±1.3} & 66.9±0.8 & 67.8±0.8 & 61.5±1.2 & 62.2±1.3 & 71.1±1.8 & 61.9±1.5 & 68.8±0.8 \\
R08 & 93.2±0.8 & 93.0±0.7 & 93.2±0.7 & 92.2±0.7 & \textbf{93.9±0.7} & 92.8±0.6 & 92.3±0.7 & 93.1±0.7 \\
R09 & 89.2±0.5 & 89.2±0.6 & 89.3±0.4 & 88.6±0.7 & \textbf{89.8±0.5} & 88.8±0.5 & 88.2±0.7 & 89.3±0.5 \\
R10 & 99.5±0.2 & 99.0±0.6 & 98.9±0.6 & 98.1±0.3 & 99.1±0.3 & \textbf{99.5±0.1} & 98.6±0.3 & 99.4±0.2 \\
R11 &  & 94.2±0.9 & 94.0±0.9 & 92.5±1.3 & \textbf{94.5±0.9} & 93.9±1.1 & 93.3±1.1 & 94.2±0.8 \\
R12 &  & 98.5±0.3 & \textbf{98.5±0.2} & 98.0±0.3 & 98.5±0.2 & 98.3±0.2 & 98.2±0.3 & 98.5±0.2 \\
R13 & 52.8±2.1 & 57.6±2.7 & \textbf{58.2±2.6} & 51.8±3.5 & 53.8±2.7 & 53.4±3.0 & 49.0±3.3 & 57.5±2.7 \\
R14 & 96.6±0.2 & 97.3±0.1 & \textbf{97.3±0.1} & 96.8±0.1 & 96.2±0.2 & 96.4±0.1 & 97.1±0.1 & 97.1±0.1 \\
R15 &  & 79.5±0.9 & 79.9±1.1 & 77.3±1.2 & 79.4±1.0 & 78.9±1.5 & 76.8±1.2 & \textbf{80.3±1.0} \\
R16 & 98.7±0.0 & 98.7±0.0 & 98.7±0.0 & 97.7±0.1 & \textbf{98.8±0.0} & 98.7±0.0 & 97.7±0.1 & 97.8±0.1 \\
R17 & 95.3±2.0 & 94.8±2.4 & 95.2±2.0 & 94.2±2.5 & 95.1±2.1 & 94.7±2.3 & 94.5±2.6 & \textbf{95.4±1.9} \\
R18 & 98.1±0.4 & 98.2±0.4 & 98.2±0.4 & 97.7±0.4 & 93.1±1.3 & 97.4±0.6 & 97.5±0.8 & \textbf{98.3±0.4} \\
R19 &  & 85.5±0.8 & \textbf{85.8±0.9} & 85.3±0.9 & 84.0±1.1 & 83.8±1.1 & 83.9±1.0 & 82.4±2.0 \\
R20 & \textbf{96.4±0.5} & 96.0±0.6 & 96.2±0.5 & 95.7±0.6 & 93.9±1.0 & 95.6±0.9 & 95.3±0.9 & 96.2±0.5 \\
R21 & 92.3±1.2 & 92.5±1.1 & 92.6±1.1 & 91.8±1.1 & \textbf{93.3±1.1} & 92.3±1.0 & 91.6±1.0 & 92.6±1.0 \\
R22 &  & 45.2±5.5 & 45.2±5.9 & 39.1±6.4 & 43.8±5.5 & 39.7±5.5 & 36.1±7.1 & \textbf{46.8±5.8} \\
R23 & 85.0±1.8 & 85.4±1.2 & \textbf{85.5±1.1} & 81.8±1.5 & 84.8±1.6 & 83.0±1.6 & 83.3±1.2 & 85.1±1.2 \\
R24 & \textbf{86.0±0.6} & 84.0±0.8 & 85.8±0.7 & 79.7±1.0 & 70.3±1.6 & 81.8±1.8 & 82.9±0.7 & 85.9±0.9 \\
R25 & 94.3±0.6 & \textbf{95.4±0.5} & 95.3±0.5 & 94.8±0.5 & 86.7±1.0 & 94.8±0.5 & 94.6±0.6 & 95.4±0.5 \\
R26 & 99.8±0.1 & 99.4±0.1 & 99.4±0.1 & 99.0±0.2 & \textbf{99.8±0.1} & 99.6±0.1 & 99.1±0.2 & 99.5±0.1 \\
R27 & 81.9±1.6 & 85.2±1.3 & 85.3±1.4 & 84.8±1.3 & 82.2±1.6 & 82.0±1.6 & 83.2±1.2 & \textbf{85.5±1.2} \\
R28 & 52.5±2.6 & 53.4±2.7 & 53.3±2.8 & 46.0±2.7 & \textbf{61.7±2.1} & 51.5±2.9 & 45.3±2.3 & 53.5±2.7 \\
R29 &  & 94.4±0.7 & 94.4±0.7 & 93.0±1.0 & \textbf{95.7±0.6} & 94.3±0.9 & 93.4±0.8 & 94.5±0.8 \\
R30 & 23.8±4.2 & 22.7±4.1 & 24.6±3.9 & 23.5±3.5 & 20.9±4.8 & 15.7±5.0 & 17.5±4.2 & \textbf{25.5±3.7} \\
R31 & 92.1±0.3 & 92.1±0.4 & 92.0±0.4 & 89.9±0.5 & \textbf{93.2±0.3} & 91.7±0.4 & 90.3±0.5 & 92.0±0.4 \\
R32 &  & 28.8±6.3 & 28.2±5.8 & 28.6±5.9 & 26.2±5.4 & 19.4±5.6 & 22.7±7.3 & \textbf{31.6±6.2} \\
R33 & 46.6±1.5 & 47.4±1.7 & 47.8±1.7 & 40.2±1.6 & 44.9±1.6 & 46.0±1.8 & 40.3±2.1 & \textbf{47.9±1.6} \\
R34 &  & 90.9±2.5 & 91.4±2.1 & 88.7±3.1 & \textbf{92.3±1.8} & 89.1±2.9 & 88.9±3.6 & 91.6±2.1 \\
R35 & \textbf{85.9±0.6} & 84.4±0.5 & 84.5±0.5 & 81.2±0.4 & 85.2±0.7 & 85.7±0.8 & 81.6±0.8 & 84.4±0.3 \\
R36 & \textbf{96.3±0.9} & 95.2±0.9 & 95.5±0.9 & 94.3±1.0 & 91.2±1.1 & 95.7±0.8 & 94.4±0.9 & 95.5±0.9 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\center
\footnotesize
\caption{Regression performance per dataset (above 10K). The top performance score is bolded first, and then all scores are rounded. We report average $R^2$ with 95\% CIs.}
\label{tab:reg_unlimit_dataset_performance}
\begin{tabular}{llllllll}
\toprule
ID & CatB-T & CatB-T-U & TabPFN & TabSTAR & TabSTAR-U & XGB-T & XGB-T-U \\
\midrule
R01 & 100±0.0 & 100±0.0 &  & 100±0.0 & \textbf{100±0.0} & 100±0.0 & 100±0.0 \\
R02 & 98.3±0.9 & 98.8±0.9 & 98.4±0.9 & 98.0±1.1 & 98.4±1.0 & 98.3±0.9 & \textbf{98.8±0.9} \\
R03 & 74.1±0.3 & \textbf{75.1±0.4} & 74.9±0.4 & 70.9±0.8 & 72.3±0.6 & 73.7±0.5 & 74.6±0.6 \\
R04 & 85.1±7.1 & 91.3±0.8 & 86.1±5.2 & 81.5±8.7 & 91.0±0.7 & 87.5±3.7 & \textbf{91.3±0.6} \\
R07 & 67.8±0.8 & 85.2±0.3 & 62.2±1.3 & 71.1±1.8 & 79.9±1.8 & 68.8±0.8 & \textbf{86.1±0.7} \\
R08 & 93.2±0.7 & 93.8±0.6 & \textbf{93.9±0.7} & 92.8±0.6 & 93.3±0.6 & 93.1±0.7 & 93.9±0.5 \\
R09 & 89.3±0.4 & 89.5±0.4 & \textbf{89.8±0.5} & 88.8±0.5 & 89.2±0.6 & 89.3±0.5 & 89.5±0.4 \\
R12 & 98.5±0.2 & \textbf{98.9±0.1} & 98.5±0.2 & 98.3±0.2 & 98.4±0.2 & 98.5±0.2 & 98.9±0.2 \\
R14 & 97.3±0.1 & \textbf{97.8±0.1} & 96.2±0.2 & 96.4±0.1 & 96.8±0.2 & 97.1±0.1 & 97.5±0.1 \\
R15 & 79.9±1.1 & 86.5±0.7 & 79.4±1.0 & 78.9±1.5 & 83.9±1.4 & 80.3±1.0 & \textbf{87.0±0.7} \\
R16 & 98.7±0.0 & \textbf{98.8±0.0} & 98.8±0.0 & 98.7±0.0 & 98.8±0.0 & 97.8±0.1 & 98.0±0.1 \\
R17 & 95.2±2.0 & 97.6±0.6 & 95.1±2.1 & 94.7±2.3 & 97.6±0.6 & 95.4±1.9 & \textbf{97.7±0.6} \\
R18 & 98.2±0.4 & \textbf{99.0±0.2} & 93.1±1.3 & 97.4±0.6 & 97.7±0.6 & 98.3±0.4 & 98.9±0.3 \\
R20 & 96.2±0.5 & 96.3±0.4 & 93.9±1.0 & 95.6±0.9 & 95.3±1.0 & 96.2±0.5 & \textbf{96.3±0.5} \\
R23 & 85.5±1.1 & \textbf{86.3±0.7} & 84.8±1.6 & 83.0±1.6 & 85.2±1.7 & 85.1±1.2 & 86.1±0.8 \\
R24 & 85.8±0.7 & 97.1±0.3 & 70.3±1.6 & 81.8±1.8 & 95.5±0.7 & 85.9±0.9 & \textbf{97.3±0.3} \\
R25 & 95.3±0.5 & \textbf{95.9±0.4} & 86.7±1.0 & 94.8±0.5 & 95.1±0.5 & 95.4±0.5 & 95.8±0.5 \\
R31 & 92.0±0.4 & 93.0±0.3 & \textbf{93.2±0.3} & 91.7±0.4 & 92.7±0.4 & 92.0±0.4 & 92.9±0.3 \\
R33 & 47.8±1.7 & 55.9±1.1 & 44.9±1.6 & 46.0±1.8 & \textbf{57.2±1.3} & 47.9±1.6 & 55.7±1.3 \\
R36 & 95.5±0.9 & 95.5±0.9 & 91.2±1.1 & \textbf{95.7±0.8} & 95.5±0.5 & 95.5±0.9 & 95.5±0.9 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Head-to-head comparisons}
\label{app:res:head_to_head}

We compare the performance of TabSTAR against each of the models in head-to-head comparisons. We report win rate, which can be seen as a private case of the normalized metric with only two models. We exclude failed runs when comparing to CARTE and TabPFN-v2. Table~\ref{tab:head_10k} shows the performance of TabSTAR against all models competing up to 10K examples, for both regression and classification, with 95\% CIs over the win rate. Table~\ref{tab:head_unlimit} does the same for TabSTAR-Unlimit.

\begin{table}
\centering
\footnotesize
\caption{Win rates of TabSTAR (up to 10K) against baselines. Win rate with 95\% CI.}
\label{tab:head_10k}
\begin{tabular}{llccccccc}
\toprule
 & CARTE & CatB & CatB-T & RF & TabPFN & XGB & XGB-T \\
\midrule
Classification & 93.3±5.2 & 73.6±7.3 & 67.9±7.8 & 88.6±5.3 & 57.3±9.3 & 89.3±5.1 & 71.4±7.5 \\
Regression & 35.7±5.9 & 33.6±4.9 & 32.2±4.8 & 66.9±4.9 & 40.9±5.2 & 69.4±4.8 & 30.3±4.8 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\footnotesize
\caption{Win rates of TabSTAR-U (above 10K) against baselines. Win rate with 95\% CI.}
\label{tab:head_unlimit}
\begin{tabular}{llccccccc}
\toprule
 & CARTE & CatB-T & CatB-T-U & TabPFN & TabSTAR & XGB-T & XGB-T-U \\
\midrule
Classification & 98.6±2.8 & 86.0±6.8 & 67.0±9.3	 & 75.7±10.1 &	94.0±4.7 & 85.0±7.0 & 71.0±8.9 \\
Regression & 63.5±7.5 & 53.0±6.9 & 22.0±5.8 & 61.1±7.0 & 82.0±5.3 & 59.5±6.8 & 28.5±6.3 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Running Times and Compute Information}
\label{app:res:run_times}

In this section we present the running times of the different models (see Table~\ref{tab:running_time}). We focus on the condition with up to 10,000 examples, excluding CatBoost-Tuned and XGBoost-Tuned as they run with a 4 hours budget using 8 CPUs from \textit{AMD EPYC 7742 64-Core Processor} .

\paragraph{Compute Information} TabSTAR runs on \textit{NVIDIA RTX A40} with 48GB memory. TabPFN-v2 runs directly on their API client. CatBoost, XGboost and Random Forest run using \textit{Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz} CPU cores. CARTE use inferior GPUs of type \textit{NVIDIA GeForce GTX 1080 Ti} with 11GB memory, to allow for higher parallelization of their incredibly slow runs. 

\paragraph{Running Times} TabSTAR's average running time for a downstream task ranges from 30 seconds (C04) to 7,692 seconds (R24). We observe that these running times are significantly lower than 14,400 seconds (4 hours), used for CatBoost-Tuned and XGBoost-Tuned, although a single run of any tree model without 5-fold cross-validation is faster. The running times of TabPFN-v2 are considerably better. However, since they use ICL, they are limited to run only up to 10,000 examples and their inference time is just as costly as their fitting time. For CARTE, while their runs use inferior hardware and limited parallelization, the reported times are for a single run. In practice, they run 6 times per dataset split because of their lack of default hyperparameters, making it a poor choice for scale.

\begin{table}
\centering
\footnotesize
\caption{Average model training running time in seconds, per dataset, for up to 10K examples.}
\label{tab:running_time}
\begin{tabular}{lllllll}
\toprule
ID & CARTE & CatBoost & RF & TabPFN-v2 & TabSTAR & XGBoost \\
\midrule
C01 & 13,341 & 44 & 19 & 42 & 302 & 31 \\
C02 &  & 420 & 346 &  & 1,145 & 491 \\
C03 & 7,297 & 38 & 19 & 61 & 324 & 27 \\
C04 &  & 11 & 6 & 9 & 30 & 9 \\
C05 & 1,227 & 10 & 8 & 14 & 68 & 9 \\
C06 & 3,068 & 68 & 31 & 112 & 1,126 & 63 \\
C07 & 6,559 & 61 & 34 & 83 & 685 & 52 \\
C08 & 4,631 & 40 & 41 & 49 & 681 & 49 \\
C09 &  & 20 & 21 & 134 & 544 & 31 \\
C10 &  & 22 & 22 & 30 & 519 & 23 \\
C11 & 2,428 & 19 & 12 & 33 & 252 & 18 \\
C12 & 4,411 & 169 & 16 &  & 765 & 39 \\
C13 & 1,774 & 31 & 27 & 58 & 929 & 37 \\
C14 &  & 500 & 140 &  & 5,254 & 308 \\
R01 & 9,439 & 139 & 446 &  & 1,987 & 94 \\
R02 &  & 9 & 16 & 54 & 188 & 6 \\
R03 & 2,174 & 30 & 103 & 357 & 553 & 24 \\
R04 &  & 10 & 10 & 84 & 115 & 7 \\
R05 & 10,639 & 28 & 26 & 62 & 158 & 15 \\
R06 & 3,185 & 22 & 43 & 70 & 307 & 13 \\
R07 & 4,637 & 22 & 109 & 112 & 352 & 28 \\
R08 & 4,252 & 57 & 366 & 396 & 1,136 & 58 \\
R09 & 8,239 & 7 & 22 & 55 & 198 & 5 \\
R10 & 1,505 & 28 & 37 & 33 & 168 & 21 \\
R11 &  & 11 & 18 & 36 & 99 & 8 \\
R12 &  & 13 & 22 & 78 & 199 & 9 \\
R13 & 2,299 & 24 & 54 & 38 & 639 & 21 \\
R14 & 6,396 & 28 & 30 & 167 & 772 & 15 \\
R15 &  & 25 & 51 & 66 & 223 & 22 \\
R16 & 13,380 & 34 & 118 & 156 & 798 & 35 \\
R17 & 3,247 & 94 & 478 & 204 & 270 & 143 \\
R18 & 9,663 & 34 & 631 & 309 & 341 & 32 \\
R19 &  & 11 & 14 & 66 & 101 & 6 \\
R20 & 6,173 & 22 & 144 & 154 & 637 & 20 \\
R21 & 2,215 & 14 & 33 & 25 & 177 & 14 \\
R22 &  & 16 & 21 & 17 & 149 & 15 \\
R23 & 14,401 & 15 & 51 & 58 & 167 & 10 \\
R24 & 25,903 & 90 & 170 & 314 & 7,692 & 106 \\
R25 & 2,404 & 50 & 943 & 261 & 786 & 54 \\
R26 & 636 & 10 & 11 & 13 & 71 & 8 \\
R27 & 2,615 & 31 & 59 & 40 & 304 & 22 \\
R28 & 12,265 & 45 & 163 & 128 & 389 & 46 \\
R29 &  & 11 & 10 & 32 & 138 & 5 \\
R30 & 2,477 & 9 & 18 & 24 & 45 & 7 \\
R31 & 5,102 & 41 & 187 & 240 & 451 & 39 \\
R32 &  & 8 & 11 & 13 & 32 & 7 \\
R33 & 4,512 & 28 & 100 & 136 & 539 & 30 \\
R34 &  & 26 & 24 & 18 & 61 & 12 \\
R35 & 4,816 & 30 & 55 & 48 & 396 & 28 \\
R36 & 15,389 & 18 & 81 & 79 & 949 & 23 \\
\bottomrule
\end{tabular}
\end{table}



\section{Extended Analysis}
\label{app:analysis}

In this section we expand on the analysis results discussed in \S\appref{sec:analysis}.

\subsection{Evaluation Datasets for Analysis}
\label{app:analysis_datasets}

All the experiments are evaluated over 20 datasets from the benchmark in Appendix~\ref{app:benchmark}. Each experiment reports the performance with AUROC for classification and $R^2$ for regression. For conciseness, each tables reports both regression and classification tasks, distinguishable by their ID.


\subsection{The Role of the Encoder Unfreezing (Q1)}
\label{app:analysis_unfreeze}

Table~\ref{tab:analysis_e5_downstream} shows the results for each variant of the experiment presented in \S\appref{analysis:e5_layers}. It is evident that unfreezing the textual encoder yields a significant performance across datasets. Furthermore, while finetuning a single layer gives a significant boost, it underperforms compared to 6 unfrozen layers. 


\subsection{The Effect of Pretraining (Q2)}
\label{app:analysis_scaling}

We pretrain three TabSTAR variants on nested dataset subsets of size 16, 64, and 256. The 64-dataset variant contains the original 16 plus 48 new datasets, and the 256-dataset variant builds on those 64 by adding another 192. This cumulative design minimizes variance between variants so that performance differences reflect only the effect of increasing data volume.

While LoRA \cite{hu_lora_2021} is a very efficient technique, its performance was much worse for a randomly initialized model. Therefore, we perform full finetuning of the non-pretrained model, as explained in Appendix~\ref{app:lora}. Table~\ref{tab:analysis_scaling_exp} shows the dataset level results. It is evident that for most of the datasets, improvement is observed when scaling, with the 256 datasets variant winning for almost all datasets.

\subsection{Numerical Verbalization (Q3)}
\label{app:analysis_numerical}

We show the full results for the experiment in \S\appref{par:numerical-verbalization}, with Table~\ref{tab:analysis_numerical_example} illustrating the verbalizations in each variant. Note that we do not include an exact value verbalization, since it would increase the number of unique text inputs and place extra memory demands. The two variants which integrate numerical information into the verbalization dominate the experiment, although the improvement seems to be marginal for some datasets. Interestingly, some datasets significantly underperform, with the \textit{R27} dataset completely failing the task. The addition of the quantile information on top of the bin seems to have limited impact, although marginally winning on the average performance.

\begin{table}
\center
\caption{Downstream performance for Q1: The Role of the Encoder Unfreezing. Results for 20 datasets with 95\% CI, for varying number of unfrozen layers. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and $R^2$ for regression.}
\label{tab:analysis_e5_downstream}
\begin{tabular}{llllll}
\toprule
ID & 0 & 1 & 3 & 6 & 9 \\
\midrule
C01 & 87.8±0.3 & 90.4±0.4 & 90.8±0.4 & \textbf{91.0±0.4} & 90.8±0.4 \\
C02 & 94.9±1.2 & 97.8±0.3 & 97.8±0.3 & \textbf{98.1±0.3} & 97.7±0.3 \\
C03 & 77.6±0.9 & 82.4±0.2 & 82.8±0.3 & \textbf{83.0±0.5} & 83.0±0.4 \\
C05 & 87.0±0.7 & 90.2±0.9 & 90.8±0.6 & \textbf{91.5±0.9} & 89.4±0.8 \\
C07 & 82.1±1.4 & 89.1±0.9 & 90.6±0.5 & \textbf{91.3±0.4} & 91.0±0.4 \\
C11 & 77.4±0.5 & 78.4±0.6 & 78.7±0.7 & \textbf{78.8±0.5} & 78.3±0.6 \\
C12 & 94.4±0.2 & \textbf{98.3±0.1} & 98.3±0.1 & 98.3±0.1 & 98.2±0.1 \\
C13 & 66.5±0.9 & 71.7±1.1 & 72.9±0.7 & \textbf{74.1±0.7} & 73.0±0.8 \\
R02 & 97.0±1.7 & 97.9±1.2 & \textbf{98.0±1.1} & 97.9±1.2 & 98.0±1.1 \\
R03 & 67.2±0.8 & 69.8±0.8 & 70.9±0.9 & \textbf{71.2±0.8} & 70.8±0.6 \\
R05 & 80.8±2.6 & 91.8±1.5 & \textbf{93.2±0.5} & 93.1±0.7 & 92.9±0.8 \\
R09 & 88.1±0.7 & 88.7±0.7 & \textbf{89.0±0.5} & 89.0±0.6 & 88.9±0.8 \\
R12 & 97.0±0.6 & 98.2±0.3 & 98.2±0.2 & \textbf{98.3±0.3} & 98.2±0.3 \\
R13 & 37.6±3.6 & 45.7±2.1 & 51.6±3.2 & 51.4±2.9 & \textbf{52.1±1.8} \\
R18 & 96.3±1.1 & 97.1±0.6 & \textbf{97.4±0.6} & 97.3±0.6 & 97.0±0.6 \\
R23 & 79.3±2.4 & 81.6±1.6 & 82.8±1.6 & \textbf{83.2±2.2} & 82.1±1.9 \\
R27 & 81.3±1.6 & 81.3±1.6 & \textbf{81.5±1.6} & 81.3±1.7 & 81.4±1.6 \\
R30 & 14.2±4.6 & 15.6±5.6 & \textbf{19.2±3.1} & 19.1±4.0 & 17.3±4.5 \\
R33 & 36.0±2.0 & 43.6±1.5 & 44.3±1.4 & \textbf{46.0±1.8} & 43.5±2.2 \\
R34 & 84.3±2.8 & 87.4±2.9 & 88.4±3.4 & 87.9±3.5 & \textbf{88.7±2.7} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\center
\caption{Downstream performance for Q2: The Effect of Pretraining. Results for 20 datasets with 95\% CI, for varying number of pretraining datasets. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and $R^2$ for regression.}
\label{tab:analysis_scaling_exp}
\begin{tabular}{lllll}
\toprule
ID & 0 & 16 & 64 & 256 \\
\midrule
C01 & 90.8±0.4 & 90.7±0.4 & 90.7±0.3 & \textbf{91.0±0.4} \\
C02 & 98.0±0.4 & 97.4±0.8 & 97.8±0.3 & \textbf{98.1±0.3} \\
C03 & 69.2±8.9 & 83.1±0.4 & \textbf{83.2±0.3} & 83.0±0.5 \\
C05 & 90.7±0.8 & 90.3±1.1 & 90.6±0.5 & \textbf{91.5±0.9} \\
C07 & 87.9±0.5 & 87.6±0.8 & 90.9±0.3 & \textbf{91.3±0.4} \\
C11 & 78.2±0.6 & 77.4±0.9 & 78.0±0.4 & \textbf{78.8±0.5} \\
C12 & \textbf{98.3±0.1} & 98.2±0.1 & 98.2±0.1 & 98.3±0.1 \\
C13 & 74.0±0.5 & 73.5±0.8 & 73.7±1.0 & \textbf{74.1±0.7} \\
R02 & 97.2±1.6 & 97.6±1.3 & 97.9±1.2 & \textbf{97.9±1.2} \\
R03 & 67.3±1.9 & 68.5±1.0 & 71.2±0.7 & \textbf{71.2±0.8} \\
R05 & 88.0±2.8 & 90.6±2.2 & 92.2±1.0 & \textbf{93.1±0.7} \\
R09 & 88.1±1.0 & 88.4±0.7 & 88.8±0.5 & \textbf{89.0±0.6} \\
R12 & 97.7±0.3 & 97.9±0.3 & 98.1±0.2 & \textbf{98.3±0.3} \\
R13 & 49.4±2.7 & 49.1±3.6 & 48.0±3.0 & \textbf{51.4±2.9} \\
R18 & 95.0±2.2 & 94.9±1.2 & 96.7±0.6 & \textbf{97.3±0.6} \\
R23 & 82.2±1.6 & 81.2±2.3 & 81.8±2.3 & \textbf{83.2±2.2} \\
R27 & 80.9±1.7 & 81.3±1.6 & \textbf{81.5±1.6} & 81.3±1.7 \\
R30 & 13.1±4.1 & 18.5±4.9 & 18.5±3.9 & \textbf{19.1±4.0} \\
R33 & 45.4±2.2 & 42.8±3.3 & 45.0±1.5 & \textbf{46.0±1.8} \\
R34 & 83.8±4.3 & 86.0±3.3 & \textbf{88.0±3.4} & 87.9±3.5 \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[h]
\centering
\caption{Illustrative verbalization of a numerical feature (\textit{Age}) for the Q3: Numerical Verbalization experiment.}
\begin{tabular}{llll}
\toprule
\textbf{Value} & \textbf{Name}      & \textbf{Name + Bin}     & \textbf{TabSTAR}          \\
\midrule
17             & Age: Numeric       & Age: Lower than 18      & Age: Lower than 18 (Quantile 0\%)        \\
20             & Age: Numeric       & Age: 18–23              & Age: 18–23 (Quantile 0–10\%)                \\
25             & Age: Numeric       & Age: 23–27              & Age: 23–27 (Quantile 10–20\%)               \\
29             & Age: Numeric       & Age: 27–31              & Age: 27–31 (Quantile 20–30\%)               \\
33             & Age: Numeric       & Age: 31–35              & Age: 31–35 (Quantile 30–40\%)               \\
38             & Age: Numeric       & Age: 35–40              & Age: 35–40 (Quantile 40–50\%)               \\
42             & Age: Numeric       & Age: 40–45              & Age: 40–45 (Quantile 50–60\%)               \\
48             & Age: Numeric       & Age: 45–51              & Age: 45–51 (Quantile 60–70\%)               \\
55             & Age: Numeric       & Age: 51–58              & Age: 51–58 (Quantile 70–80\%)               \\
63             & Age: Numeric       & Age: 58–67              & Age: 58–67 (Quantile 80–90\%)               \\

83             & Age: Numeric       & Age: 67-87     & Age: 67–87 (Quantile 90–100\%)     \\

93             & Age: Numeric       & Age: Higher than 87     & Age: Higher than 87 (Quantile 100\%)     \\
–              & Age: Unknown Value & Age: Unknown Value      & Age: Unknown Value                          \\
\bottomrule
\end{tabular}
\label{tab:analysis_numerical_example}
\end{table}



\begin{table}
\center
\caption{Downstream performance for Q3: Numerical Verbalization. Results for 20 datasets with 95\% CI, for different verbalizations. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and $R^2$ for regression.}
\label{tab:analysis_numerical_exp}
\begin{tabular}{llll}
\toprule
ID & Name & Name + Bin & TabSTAR \\
\midrule
C01 & 91.0±0.4 & \textbf{91.2±0.4} & 91.0±0.4 \\
C02 & \textbf{98.1±0.2} & 97.9±0.3 & 98.1±0.3 \\
C03 & 83.3±0.4 & \textbf{83.3±0.3} & 83.0±0.5 \\
C05 & 90.8±0.9 & 91.1±1.2 & \textbf{91.5±0.9} \\
C07 & 91.2±0.2 & \textbf{91.4±0.4} & 91.3±0.4 \\
C11 & 78.2±0.5 & 78.2±0.6 & \textbf{78.8±0.5} \\
C12 & 98.2±0.1 & \textbf{98.4±0.1} & 98.3±0.1 \\
C13 & 71.6±0.7 & 73.8±0.9 & \textbf{74.1±0.7} \\
R02 & \textbf{98.0±1.1} & 98.0±1.1 & 97.9±1.2 \\
R03 & 67.4±0.7 & \textbf{71.3±0.6} & 71.2±0.8 \\
R05 & 93.1±1.1 & \textbf{93.2±0.8} & 93.1±0.7 \\
R09 & 88.9±0.5 & 88.9±0.6 & \textbf{89.0±0.6} \\
R12 & 98.2±0.2 & \textbf{98.3±0.2} & 98.3±0.3 \\
R13 & 50.1±3.3 & 49.7±3.4 & \textbf{51.4±2.9} \\
R18 & 97.1±0.6 & 97.1±0.7 & \textbf{97.3±0.6} \\
R23 & 81.9±2.4 & 82.7±1.9 & \textbf{83.2±2.2} \\
R27 & 16.7±6.6 & \textbf{82.0±1.5} & 81.3±1.7 \\
R30 & 16.4±4.4 & 17.7±5.3 & \textbf{19.1±4.0} \\
R33 & 45.6±1.1 & \textbf{46.2±2.2} & 46.0±1.8 \\
R34 & 88.0±3.6 & \textbf{88.6±2.5} & 87.9±3.5 \\
\bottomrule
\end{tabular}
\end{table}